"""
ìµœì¢… PDF ì²˜ë¦¬ ë° ì„ë² ë”© íŒŒì´í”„ë¼ì¸
ë‹¹ì‹ ì˜ embeddings.pyì™€ vectorstore.pyì™€ 100% í˜¸í™˜

êµ¬ì¡°:
1. PDF ë‹¤ìš´ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
2. í…ìŠ¤íŠ¸ ì²­í‚¹
3. ì²­í¬ ì„ë² ë”© (ë°°ì¹˜ ì²˜ë¦¬)
4. ChromaDB ì €ì¥

ì‚¬ìš© ë°©ë²•:
from app.tools.embeddings import SentenceTransformerEmbedding
from app.tools.vectorstore import ArxivPaperVectorStore
from app.tools.pdf_embedding_pipeline import PDFEmbeddingPipeline

embedding_model = SentenceTransformerEmbedding()
vectorstore = ArxivPaperVectorStore()
pipeline = PDFEmbeddingPipeline(embedding_model, vectorstore)

# ë…¼ë¬¸ ì²˜ë¦¬
result = pipeline.process_paper('2401.00001', paper_metadata)
"""

import logging
import requests
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import time
import os

logger = logging.getLogger(__name__)


class PDFDownloadAndExtract:
    """arXivì—ì„œ PDFë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    
    def __init__(self, cache_dir: str = "./data/arxiv_pdfs"):
        """
        ì´ˆê¸°í™”
        
        Args:
            cache_dir: PDFë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # PDF ì¶”ì¶œ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
        self.use_pdfplumber = self._check_pdfplumber()
        self.use_pypdf = self._check_pypdf()
        
        if not self.use_pdfplumber and not self.use_pypdf:
            logger.warning("âš ï¸ PDF ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ. ì„¤ì¹˜: pip install pdfplumber")
    
    def _check_pdfplumber(self) -> bool:
        """pdfplumber ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import pdfplumber
            logger.info("âœ“ pdfplumber ì‚¬ìš© ê°€ëŠ¥")
            return True
        except ImportError:
            return False
    
    def _check_pypdf(self) -> bool:
        """PyPDF ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        try:
            import pypdf
            logger.info("âœ“ PyPDF ì‚¬ìš© ê°€ëŠ¥")
            return True
        except ImportError:
            return False
    
    def download_pdf(self, arxiv_id: str) -> Optional[str]:
        """
        arXivì—ì„œ PDFë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.
        
        Args:
            arxiv_id: arXiv ë…¼ë¬¸ ID (ì˜ˆ: 2401.00001)
        
        Returns:
            PDF íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None
        """
        
        arxiv_id = arxiv_id.strip().replace('/', '')
        cache_file = self.cache_dir / f"{arxiv_id}.pdf"
        
        # ì´ë¯¸ ë‹¤ìš´ë¡œë“œë˜ì—ˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
        if cache_file.exists():
            logger.debug(f"ìºì‹œëœ PDF ì‚¬ìš©: {arxiv_id}")
            return str(cache_file)
        
        pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
        
        try:
            logger.info(f"PDF ë‹¤ìš´ë¡œë“œ: {arxiv_id}")
            
            response = requests.get(pdf_url, timeout=30)
            response.raise_for_status()
            
            with open(cache_file, 'wb') as f:
                f.write(response.content)
            
            file_size = cache_file.stat().st_size / (1024 * 1024)
            logger.info(f"âœ“ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {file_size:.2f}MB")
            
            return str(cache_file)
        
        except Exception as e:
            logger.error(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def extract_text(self, pdf_path: str, max_pages: Optional[int] = None) -> Optional[str]:
        """
        PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            max_pages: ìµœëŒ€ ì¶”ì¶œ í˜ì´ì§€ (Noneì´ë©´ ì „ì²´)
        
        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë˜ëŠ” None
        """
        
        if not os.path.exists(pdf_path):
            logger.error(f"PDF íŒŒì¼ ì—†ìŒ: {pdf_path}")
            return None
        
        try:
            if self.use_pdfplumber:
                return self._extract_pdfplumber(pdf_path, max_pages)
            elif self.use_pypdf:
                return self._extract_pypdf(pdf_path, max_pages)
            else:
                logger.error("ì‚¬ìš© ê°€ëŠ¥í•œ PDF ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
                return None
        
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _extract_pdfplumber(self, pdf_path: str, max_pages: Optional[int]) -> Optional[str]:
        """pdfplumberë¥¼ ì‚¬ìš©í•œ ì¶”ì¶œ"""
        try:
            import pdfplumber
            
            text_parts = []
            
            with pdfplumber.open(pdf_path) as pdf:
                total_pages = len(pdf.pages)
                pages_to_extract = total_pages if max_pages is None else min(max_pages, total_pages)
                
                logger.info(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘: {pages_to_extract}/{total_pages} í˜ì´ì§€")
                
                for i, page in enumerate(pdf.pages[:pages_to_extract]):
                    try:
                        page_text = page.extract_text()
                        if page_text:
                            text_parts.append(page_text)
                    except Exception as e:
                        logger.warning(f"í˜ì´ì§€ {i+1} ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            
            text = "\n\n".join(text_parts)
            logger.info(f"âœ“ ì¶”ì¶œ ì™„ë£Œ: {len(text)} ê¸€ì")
            
            return text if text.strip() else None
        
        except Exception as e:
            logger.error(f"pdfplumber ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _extract_pypdf(self, pdf_path: str, max_pages: Optional[int]) -> Optional[str]:
        """PyPDFë¥¼ ì‚¬ìš©í•œ ì¶”ì¶œ"""
        try:
            from pypdf import PdfReader
            
            text_parts = []
            reader = PdfReader(pdf_path)
            total_pages = len(reader.pages)
            pages_to_extract = total_pages if max_pages is None else min(max_pages, total_pages)
            
            logger.info(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘: {pages_to_extract}/{total_pages} í˜ì´ì§€")
            
            for i in range(pages_to_extract):
                try:
                    page_text = reader.pages[i].extract_text()
                    if page_text:
                        text_parts.append(page_text)
                except Exception as e:
                    logger.warning(f"í˜ì´ì§€ {i+1} ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            
            text = "\n\n".join(text_parts)
            logger.info(f"âœ“ ì¶”ì¶œ ì™„ë£Œ: {len(text)} ê¸€ì")
            
            return text if text.strip() else None
        
        except Exception as e:
            logger.error(f"PyPDF ì‹¤íŒ¨: {str(e)}")
            return None


class SimpleTextChunker:
    """
    ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì²­í‚¹
    
    ë…¼ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ìˆëŠ” í¬ê¸°ì˜ ì²­í¬ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
    """
    
    def __init__(self, chunk_chars: int = 1800, overlap_chars: int = 350):
        """
        ì´ˆê¸°í™”
        
        Args:
            chunk_chars: ì²­í¬ì˜ ëª©í‘œ ë¬¸ì ìˆ˜
                        (Sentence TransformersëŠ” ì•½ 512 í† í°ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë©°,
                         ì˜ì–´ ê¸°ì¤€ 1 í† í° â‰ˆ 4 ë¬¸ìì´ë¯€ë¡œ chunk_chars â‰ˆ 2000)
            overlap_chars: ì²­í¬ ê°„ ì˜¤ë²„ë˜í”„ ë¬¸ì ìˆ˜
        """
        self.chunk_chars = chunk_chars
        self.overlap_chars = overlap_chars
        logger.info(f"âœ“ TextChunker ì´ˆê¸°í™”: {chunk_chars} ë¬¸ì, {overlap_chars} ì˜¤ë²„ë˜í”„")
    
    def chunk(self, text: str, arxiv_id: str = "") -> List[Dict]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
        
        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸
            arxiv_id: ë…¼ë¬¸ ID (ë©”íƒ€ë°ì´í„°ìš©)
        
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸:
            {
                'chunk_id': str,
                'content': str,
                'chunk_index': int
            }
        """
        
        if not text or not text.strip():
            return []
        
        # ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬ (ê°„ë‹¨í•œ ë°©ì‹)
        sentences = self._split_sentences(text)
        
        chunks = []
        current_chunk = ""
        chunk_index = 0
        
        for sentence in sentences:
            sentence = sentence.strip()
            
            if not sentence:
                continue
            
            # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ì„ ì¶”ê°€í–ˆì„ ë•Œì˜ ê¸¸ì´ ê³„ì‚°
            test_chunk = current_chunk + " " + sentence if current_chunk else sentence
            
            # ì²­í¬ í¬ê¸° ì´ˆê³¼ ì‹œ ì €ì¥
            if len(test_chunk) > self.chunk_chars and current_chunk:
                chunks.append({
                    'chunk_id': f"{arxiv_id}_chunk_{chunk_index}" if arxiv_id else f"chunk_{chunk_index}",
                    'content': current_chunk.strip(),
                    'chunk_index': chunk_index
                })
                
                chunk_index += 1
                
                # ì˜¤ë²„ë˜í”„ë¥¼ ìœ„í•´ ì´ì „ ë‚´ìš©ì˜ ì¼ë¶€ ìœ ì§€
                sentences_in_chunk = current_chunk.split('. ')
                if len(sentences_in_chunk) > 1:
                    overlap_text = '. '.join(sentences_in_chunk[-2:])
                else:
                    overlap_text = sentences_in_chunk[0]
                
                current_chunk = overlap_text + " " + sentence
            else:
                current_chunk = test_chunk
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk.strip():
            chunks.append({
                'chunk_id': f"{arxiv_id}_chunk_{chunk_index}" if arxiv_id else f"chunk_{chunk_index}",
                'content': current_chunk.strip(),
                'chunk_index': chunk_index
            })
        
        logger.info(f"âœ“ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
        
        return chunks
    
    def _split_sentences(self, text: str) -> List[str]:
        """ê°„ë‹¨í•œ ë¬¸ì¥ ë¶„ë¦¬"""
        import re
        
        # ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œë¡œ ë¬¸ì¥ ë¶„ë¦¬
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        return sentences


class PDFEmbeddingPipeline:
    """
    ì™„ì „í•œ PDF ì„ë² ë”© íŒŒì´í”„ë¼ì¸
    
    ë‹¹ì‹ ì˜ SentenceTransformerEmbeddingê³¼ ArxivPaperVectorStoreì™€ í˜¸í™˜ë©ë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        embedding_model,  # SentenceTransformerEmbedding ì¸ìŠ¤í„´ìŠ¤
        vectorstore,  # ArxivPaperVectorStore ì¸ìŠ¤í„´ìŠ¤
        chunk_chars: int = 1800,
        overlap_chars: int = 350,
        batch_size: int = 32
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            embedding_model: SentenceTransformerEmbedding ì¸ìŠ¤í„´ìŠ¤
            vectorstore: ArxivPaperVectorStore ì¸ìŠ¤í„´ìŠ¤
            chunk_chars: ì²­í¬ ë¬¸ì ìˆ˜
            overlap_chars: ì˜¤ë²„ë˜í”„ ë¬¸ì ìˆ˜
            batch_size: ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°
        """
        
        self.embedding_model = embedding_model
        self.vectorstore = vectorstore
        self.batch_size = batch_size
        
        self.pdf_processor = PDFDownloadAndExtract()
        self.chunker = SimpleTextChunker(
            chunk_chars=chunk_chars,
            overlap_chars=overlap_chars
        )
        
        logger.info("âœ“ PDFEmbeddingPipeline ì´ˆê¸°í™” ì™„ë£Œ")
    
    def process_paper(
        self,
        arxiv_id: str,
        paper_metadata: Dict,
        max_pages: int = 10
    ) -> Dict:
        """
        ë‹¨ì¼ ë…¼ë¬¸ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            arxiv_id: arXiv ID
            paper_metadata: ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°
            max_pages: ìµœëŒ€ ì²˜ë¦¬ í˜ì´ì§€
        
        Returns:
            ì²˜ë¦¬ ê²°ê³¼:
            {
                'success': bool,
                'chunks_created': int,
                'chunks_embedded': int,
                'chunks_saved': int,
                'message': str,
                'time': float
            }
        """
        
        start_time = time.time()
        
        logger.info("="*60)
        logger.info(f"[ì²˜ë¦¬ ì‹œì‘] {arxiv_id}")
        logger.info("="*60)
        
        try:
            # 1ë‹¨ê³„: PDF ë‹¤ìš´ë¡œë“œ
            logger.info("1ë‹¨ê³„: PDF ë‹¤ìš´ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ...")
            
            pdf_path = self.pdf_processor.download_pdf(arxiv_id)
            
            if not pdf_path:
                return {
                    "success": False,
                    "arxiv_id": arxiv_id,
                    "chunks_created": 0,
                    "chunks_embedded": 0,
                    "chunks_saved": 0,
                    "message": "PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨",
                    "time": time.time() - start_time
                }
            
            # 2ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = self.pdf_processor.extract_text(pdf_path, max_pages=max_pages)
            
            if not text:
                return {
                    "success": False,
                    "arxiv_id": arxiv_id,
                    "chunks_created": 0,
                    "chunks_embedded": 0,
                    "chunks_saved": 0,
                    "message": "í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨",
                    "time": time.time() - start_time
                }
            
            logger.info(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(text)} ê¸€ì")
            
            # 3ë‹¨ê³„: ì²­í‚¹
            logger.info("2ë‹¨ê³„: í…ìŠ¤íŠ¸ ì²­í‚¹...")
            
            chunks = self.chunker.chunk(text, arxiv_id=arxiv_id)
            
            if not chunks:
                return {
                    "success": False,
                    "arxiv_id": arxiv_id,
                    "chunks_created": 0,
                    "chunks_embedded": 0,
                    "chunks_saved": 0,
                    "message": "ì²­í‚¹ ì‹¤íŒ¨",
                    "time": time.time() - start_time
                }
            
            # 4ë‹¨ê³„: ì„ë² ë”© (ë°°ì¹˜ ì²˜ë¦¬)
            logger.info("3ë‹¨ê³„: ì²­í¬ ì„ë² ë”© (ë°°ì¹˜ ì²˜ë¦¬)...")
            
            chunk_texts = [chunk['content'] for chunk in chunks]
            
            try:
                # ë‹¹ì‹ ì˜ embedding_modelì˜ embed_batch ì‚¬ìš©
                embeddings = self.embedding_model.embed_batch(chunk_texts)
                logger.info(f"âœ“ {len(embeddings)}ê°œ ì²­í¬ ì„ë² ë”© ì™„ë£Œ")
            except Exception as e:
                logger.error(f"ì„ë² ë”© ì‹¤íŒ¨: {str(e)}")
                return {
                    "success": False,
                    "arxiv_id": arxiv_id,
                    "chunks_created": len(chunks),
                    "chunks_embedded": 0,
                    "chunks_saved": 0,
                    "message": f"ì„ë² ë”© ì‹¤íŒ¨: {str(e)}",
                    "time": time.time() - start_time
                }
            
            # 5ë‹¨ê³„: ChromaDB ì €ì¥
            logger.info("4ë‹¨ê³„: ChromaDBì— ì €ì¥...")
            
            saved_count = self._save_to_vectorstore(
                chunks,
                embeddings,
                arxiv_id,
                paper_metadata
            )
            
            elapsed = time.time() - start_time
            
            logger.info(f"âœ“ ì²˜ë¦¬ ì™„ë£Œ: {saved_count}ê°œ ì²­í¬ ì €ì¥ ({elapsed:.2f}ì´ˆ)")
            
            return {
                "success": True,
                "arxiv_id": arxiv_id,
                "chunks_created": len(chunks),
                "chunks_embedded": len(embeddings),
                "chunks_saved": saved_count,
                "message": f"{saved_count}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ",
                "time": elapsed
            }
        
        except Exception as e:
            logger.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
            
            return {
                "success": False,
                "arxiv_id": arxiv_id,
                "chunks_created": 0,
                "chunks_embedded": 0,
                "chunks_saved": 0,
                "message": f"ì˜¤ë¥˜: {str(e)}",
                "time": time.time() - start_time
            }
    
    def _save_to_vectorstore(
        self,
        chunks: List[Dict],
        embeddings: List[List[float]],
        arxiv_id: str,
        paper_metadata: Dict
    ) -> int:
        """
        ì²­í¬ì™€ ì„ë² ë”©ì„ ChromaDBì— ì €ì¥í•©ë‹ˆë‹¤.
        ë‹¹ì‹ ì˜ vectorstore.collectionì„ ì§ì ‘ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        
        try:
            ids = [chunk['chunk_id'] for chunk in chunks]
            documents = [chunk['content'] for chunk in chunks]
            
            # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            metadatas = []
            for chunk in chunks:
                metadata = {
                    'arxiv_id': arxiv_id,
                    'chunk_index': str(chunk['chunk_index']),
                    'title': paper_metadata.get('title', ''),
                    'authors': ', '.join(paper_metadata.get('authors', [])),
                }
                metadatas.append(metadata)
            
            # ChromaDB ì»¬ë ‰ì…˜ì— ì§ì ‘ ì €ì¥
            self.vectorstore.collection.add(
                ids=ids,
                documents=documents,
                metadatas=metadatas,
                embeddings=embeddings
            )
            
            logger.info(f"âœ“ {len(ids)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ")
            
            return len(ids)
        
        except Exception as e:
            logger.error(f"ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return 0
    
    def process_papers_batch(
        self,
        papers: List[Dict],
        max_pages: int = 10
    ) -> Dict:
        """
        ì—¬ëŸ¬ ë…¼ë¬¸ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            papers: ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
                ê° í•­ëª©:
                {
                    'arxiv_id': str,
                    'title': str,
                    'authors': List[str],
                    ...
                }
            max_pages: ìµœëŒ€ ì²˜ë¦¬ í˜ì´ì§€
        
        Returns:
            ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼
        """
        
        logger.info("="*60)
        logger.info(f"[ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘] {len(papers)}ê°œ ë…¼ë¬¸")
        logger.info("="*60)
        
        results = []
        successful = 0
        total_chunks = 0
        start_time = time.time()
        
        for i, paper in enumerate(papers):
            arxiv_id = paper.get('arxiv_id')
            
            logger.info(f"\n[{i+1}/{len(papers)}] {arxiv_id} ì²˜ë¦¬ ì¤‘...")
            
            result = self.process_paper(
                arxiv_id=arxiv_id,
                paper_metadata=paper,
                max_pages=max_pages
            )
            
            results.append(result)
            
            if result['success']:
                successful += 1
                total_chunks += result['chunks_saved']
            
            # API ìš”ì²­ ì‚¬ì´ì— ì ê¹ ëŒ€ê¸° (arXiv ì„œë²„ ë¶€í•˜ ê³ ë ¤)
            if i < len(papers) - 1:
                time.sleep(2)
        
        total_time = time.time() - start_time
        
        logger.info("\n" + "="*60)
        logger.info("[ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ]")
        logger.info("="*60)
        logger.info(f"ì„±ê³µ: {successful}/{len(papers)}")
        logger.info(f"ì´ ì €ì¥ëœ ì²­í¬: {total_chunks}")
        logger.info(f"ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
        
        return {
            "total": len(papers),
            "successful": successful,
            "failed": len(papers) - successful,
            "total_chunks": total_chunks,
            "time": total_time,
            "results": results,
            "message": f"{successful}ê°œ ë…¼ë¬¸ ì²˜ë¦¬ ì™„ë£Œ, {total_chunks}ê°œ ì²­í¬ ì €ì¥"
        }


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    print("\n" + "="*60)
    print("ğŸš€ PDF ì„ë² ë”© íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸")
    print("="*60 + "\n")
    
    try:
        from embeddings import SentenceTransformerEmbedding
        from vectorstore import ArxivPaperVectorStore
        
        # ì´ˆê¸°í™”
        embedding_model = SentenceTransformerEmbedding(
            model_name="distiluse-base-multilingual-cased-v2"
        )
        
        vectorstore = ArxivPaperVectorStore(
            persist_directory="./data/arxiv_chunks",
            collection_name="arxiv_chunks"
        )
        
        pipeline = PDFEmbeddingPipeline(
            embedding_model=embedding_model,
            vectorstore=vectorstore
        )
        
        # í…ŒìŠ¤íŠ¸
        test_papers = [
            {
                'arxiv_id': '2401.01111',
                'title': 'Test Paper',
                'authors': ['Author1']
            }
        ]
        
        result = pipeline.process_papers_batch(test_papers, max_pages=2)
        print(f"\nê²°ê³¼: {result['message']}")
    
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {str(e)}")
        print("embeddings.pyì™€ vectorstore.pyê°€ í•„ìš”í•©ë‹ˆë‹¤")