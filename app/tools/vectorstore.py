"""
ARA í”„ë¡œì íŠ¸ë¥¼ ìœ„í•œ ChromaDB ë²¡í„° ìŠ¤í† ì–´ í†µí•© ëª¨ë“ˆ
ë‹¹ì‹ ì˜ workflow.pyì™€ embeddings.pyì™€ ì™„ë²½í•˜ê²Œ í˜¸í™˜ë©ë‹ˆë‹¤.

êµ¬ì¡°:
1. ArxivPaperVectorStore: ë…¼ë¬¸ ë°ì´í„° ê´€ë¦¬
2. SemanticSearchEngine: ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì—”ì§„
3. WorkflowIntegration: workflowì™€ì˜ í†µí•© ë ˆì´ì–´
"""

import chromadb
from chromadb.config import Settings
import logging
from typing import List, Dict, Optional, Tuple
from pathlib import Path
import json
from datetime import datetime
import numpy as np

# ë‹¹ì‹ ì˜ embeddings ëª¨ë“ˆì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ import
try:
    from tools.embeddings import embed_text, calculate_semantic_similarity
except ImportError:
    # ê°œë°œ í™˜ê²½ì—ì„œì˜ í´ë°±
    embed_text = None
    calculate_semantic_similarity = None

logger = logging.getLogger(__name__)


class ArxivPaperVectorStore:
    """
    arXiv ë…¼ë¬¸ì„ ìœ„í•œ ChromaDB ë²¡í„° ìŠ¤í† ì–´
    
    ì´ í´ë˜ìŠ¤ëŠ” ë…¼ë¬¸ì˜ ë©”íƒ€ë°ì´í„°ì™€ ì„ë² ë”©ì„ ì €ì¥í•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤.
    ë‹¹ì‹ ì˜ embeddings.pyì˜ embed_text í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    
    ì €ì¥ë˜ëŠ” ì •ë³´:
    - ë…¼ë¬¸ IDì™€ ì œëª©
    - ì´ˆë¡ (abstract)
    - ì €ì ì •ë³´
    - ì¹´í…Œê³ ë¦¬
    - ë°œí‘œ ë‚ ì§œ
    - arXiv ë§í¬ ë“±
    
    ëª¨ë“  ë©”íƒ€ë°ì´í„°ëŠ” ChromaDBì˜ í•„í„°ë§ ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        persist_directory: str = "./data/arxiv_vectorstore",
        collection_name: str = "arxiv_papers"
    ):
        """
        VectorStore ì´ˆê¸°í™”
        
        Args:
            persist_directory: ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
            collection_name: ì»¬ë ‰ì…˜ ì´ë¦„
        """
        self.persist_directory = persist_directory
        self.collection_name = collection_name
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        Path(persist_directory).mkdir(parents=True, exist_ok=True)
        
        # ChromaDB ì´ˆê¸°í™”
        settings = Settings(
            chroma_db_impl="duckdb+parquet",
            persist_directory=persist_directory,
            anonymized_telemetry=False
        )
        
        self.client = chromadb.Client(settings)
        
        # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ì—°ê²°
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}
        )
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬
        self.log_dir = Path(persist_directory) / "logs"
        self.log_dir.mkdir(exist_ok=True)
        
        logger.info(f"âœ“ ArxivPaperVectorStore ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"  - ë””ë ‰í† ë¦¬: {persist_directory}")
        logger.info(f"  - ì»¬ë ‰ì…˜: {collection_name}")
    
    def add_papers_from_arxiv_search(self, arxiv_papers: List[Dict]) -> Dict:
        """
        arXiv APIì—ì„œ ê²€ìƒ‰í•œ ë…¼ë¬¸ë“¤ì„ VectorStoreì— ì¶”ê°€í•©ë‹ˆë‹¤.
        
        ì´ ë©”ì„œë“œëŠ” workflowì˜ search_papers_nodeì—ì„œ ë°˜í™˜ëœ
        ë…¼ë¬¸ ëª©ë¡ì„ ë°›ì•„ VectorStoreì— ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            arxiv_papers: arXiv APIì—ì„œ ë°˜í™˜í•œ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
                êµ¬ì¡°:
                {
                    'arxiv_id': '2401.00001',
                    'title': 'Paper Title',
                    'abstract': 'Paper abstract...',
                    'authors': ['Author1', 'Author2'],
                    'categories': ['cs.LG', 'cs.AI'],
                    'published_date': '2024-01-01',
                    'pdf_url': 'https://...',
                    'html_url': 'https://...'
                }
        
        Returns:
            {
                'success': bool,
                'added_count': int,
                'message': str
            }
        """
        
        if not arxiv_papers:
            return {
                "success": False,
                "added_count": 0,
                "message": "ì¶”ê°€í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤"
            }
        
        try:
            logger.info(f"ë…¼ë¬¸ ì¶”ê°€ ì‹œì‘: {len(arxiv_papers)}ê°œ")
            
            ids = []
            documents = []
            metadatas = []
            embeddings = []
            
            for i, paper in enumerate(arxiv_papers):
                arxiv_id = paper.get('arxiv_id')
                
                if not arxiv_id:
                    logger.warning(f"ë…¼ë¬¸ {i}: arxiv_id ì—†ìŒ, ê±´ë„ˆëœ€")
                    continue
                
                # ê³ ìœ  ID ìƒì„±
                doc_id = f"arxiv_{arxiv_id.replace('.', '_')}"
                
                # ë¬¸ì„œ ë‚´ìš©: ì œëª© + ì´ˆë¡
                title = paper.get('title', '')
                abstract = paper.get('abstract', '')
                content = f"{title}\n\n{abstract}".strip()
                
                if not content:
                    logger.warning(f"ë…¼ë¬¸ {arxiv_id}: ì œëª©/ì´ˆë¡ ì—†ìŒ, ê±´ë„ˆëœ€")
                    continue
                
                # ë‹¹ì‹ ì˜ embed_text í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ìƒì„±
                try:
                    if embed_text is not None:
                        embedding = embed_text(content)
                        # numpy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        if hasattr(embedding, 'tolist'):
                            embedding = embedding.tolist()
                    else:
                        logger.warning("embed_textë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                        embedding = None
                except Exception as e:
                    logger.error(f"ë…¼ë¬¸ {arxiv_id} ì„ë² ë”© ì‹¤íŒ¨: {str(e)}")
                    embedding = None
                
                # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
                metadata = {
                    'arxiv_id': arxiv_id,
                    'title': title,
                    'authors': ', '.join(paper.get('authors', [])),
                    'categories': ', '.join(paper.get('categories', [])),
                    'published_date': paper.get('published_date', ''),
                    'pdf_url': paper.get('pdf_url', ''),
                    'html_url': paper.get('html_url', '')
                }
                
                # ë°ì´í„° ìˆ˜ì§‘
                ids.append(doc_id)
                documents.append(content)
                metadatas.append(metadata)
                if embedding:
                    embeddings.append(embedding)
                
                if (i + 1) % 10 == 0:
                    logger.info(f"  {i + 1}/{len(arxiv_papers)} ì„ë² ë”© ì™„ë£Œ")
            
            if not ids:
                return {
                    "success": False,
                    "added_count": 0,
                    "message": "ìœ íš¨í•œ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤"
                }
            
            # ChromaDBì— ì¶”ê°€
            if embeddings and len(embeddings) == len(ids):
                self.collection.add(
                    ids=ids,
                    documents=documents,
                    metadatas=metadatas,
                    embeddings=embeddings
                )
            else:
                # ì¼ë¶€ ì„ë² ë”©ì´ ëˆ„ë½ëœ ê²½ìš°
                self.collection.add(
                    ids=ids,
                    documents=documents,
                    metadatas=metadatas
                )
            
            logger.info(f"âœ“ {len(ids)}ê°œ ë…¼ë¬¸ ì¶”ê°€ ì™„ë£Œ")
            
            # ë¡œê·¸ ì €ì¥
            self._save_operation_log(ids, 'add_papers', len(ids))
            
            return {
                "success": True,
                "added_count": len(ids),
                "message": f"{len(ids)}ê°œ ë…¼ë¬¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤"
            }
        
        except Exception as e:
            logger.error(f"ë…¼ë¬¸ ì¶”ê°€ ì‹¤íŒ¨: {str(e)}")
            return {
                "success": False,
                "added_count": 0,
                "message": f"ì˜¤ë¥˜: {str(e)}"
            }
    
    def get_collection_count(self) -> int:
        """ì €ì¥ëœ ë…¼ë¬¸ì˜ ì´ ê°œìˆ˜"""
        try:
            return self.collection.count()
        except Exception as e:
            logger.error(f"ì»¬ë ‰ì…˜ í¬ê¸° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return 0
    
    def _save_operation_log(self, doc_ids: List[str], operation: str, count: int):
        """ì‘ì—… ë¡œê·¸ ì €ì¥"""
        try:
            log_file = self.log_dir / "operations.jsonl"
            log_entry = {
                "timestamp": datetime.now().isoformat(),
                "operation": operation,
                "count": count,
                "doc_ids_sample": doc_ids[:5]
            }
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
        except Exception as e:
            logger.warning(f"ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {str(e)}")


class SemanticSearchEngine:
    """
    ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì—”ì§„
    
    ì´ í´ë˜ìŠ¤ëŠ” ë‹¹ì‹ ì˜ embeddings.pyì˜ calculate_semantic_similarityë¥¼
    ì‚¬ìš©í•˜ì—¬ ë…¼ë¬¸ë“¤ì„ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
    
    workflowì˜ evaluate_relevance_nodeì—ì„œ ì´ ì—”ì§„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, vectorstore: ArxivPaperVectorStore):
        """
        ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        
        Args:
            vectorstore: ArxivPaperVectorStore ì¸ìŠ¤í„´ìŠ¤
        """
        self.vectorstore = vectorstore
        self.min_similarity_threshold = 0.3  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
    
    def evaluate_papers_semantic_relevance(
        self,
        papers: List[Dict],
        query: str,
        top_k: Optional[int] = None,
        threshold: float = 0.3
    ) -> List[Dict]:
        """
        ê²€ìƒ‰ëœ ë…¼ë¬¸ë“¤ì˜ ì˜ë¯¸ì  ê´€ë ¨ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤.
        
        ì´ê²ƒì´ workflowì˜ evaluate_relevance_nodeì—ì„œ ì‚¬ìš©ë˜ëŠ” í•µì‹¬ ë©”ì„œë“œì…ë‹ˆë‹¤.
        
        Args:
            papers: í‰ê°€í•  ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
            query: ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸ (ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚° ê¸°ì¤€)
            top_k: ë°˜í™˜í•  ìƒìœ„ ë…¼ë¬¸ ê°œìˆ˜ (Noneì´ë©´ ëª¨ë‘ ë°˜í™˜)
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (ì´ ì´ìƒì¸ ë…¼ë¬¸ë§Œ ë°˜í™˜)
        
        Returns:
            ìœ ì‚¬ë„ ì ìˆ˜ê°€ ì¶”ê°€ëœ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸ (ìœ ì‚¬ë„ ìˆœ ë‚´ë¦¼ì°¨ìˆœ)
            
            êµ¬ì¡°:
            {
                'arxiv_id': '2401.00001',
                'title': 'Paper Title',
                'abstract': 'Paper abstract...',
                'authors': [...],
                'categories': [...],
                'semantic_score': 0.75,  # ì¶”ê°€ëœ í•„ë“œ
                'content_for_summary': 'ì œëª©\n\nì´ˆë¡...'
            }
        """
        
        if not papers:
            logger.warning("í‰ê°€í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        logger.info(f"ì˜ë¯¸ ê¸°ë°˜ í‰ê°€ ì‹œì‘: {len(papers)}ê°œ ë…¼ë¬¸, ì¿¼ë¦¬: {query[:50]}...")
        
        try:
            evaluated_papers = []
            
            for paper in papers:
                # ë…¼ë¬¸ì˜ ì œëª©ê³¼ ì´ˆë¡ì„ ê²°í•©í•˜ì—¬ í‰ê°€ ëŒ€ìƒ í…ìŠ¤íŠ¸ ìƒì„±
                title = paper.get('title', '')
                abstract = paper.get('abstract', '')
                paper_content = f"{title}\n\n{abstract}".strip()
                
                if not paper_content:
                    logger.warning(f"ë…¼ë¬¸ {paper.get('arxiv_id')}: í‰ê°€ í…ìŠ¤íŠ¸ ì—†ìŒ")
                    continue
                
                # ë‹¹ì‹ ì˜ calculate_semantic_similarity í•¨ìˆ˜ ì‚¬ìš©
                # ì´ í•¨ìˆ˜ëŠ” 0~1 ë²”ìœ„ì˜ ì •ê·œí™”ëœ ìœ ì‚¬ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤
                try:
                    if calculate_semantic_similarity is not None:
                        semantic_score = calculate_semantic_similarity(query, paper_content)
                    else:
                        logger.warning("calculate_semantic_similarityë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                        semantic_score = 0.0
                except Exception as e:
                    logger.error(f"ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨ ({paper.get('arxiv_id')}): {str(e)}")
                    semantic_score = 0.0
                
                # ì„ê³„ê°’ ì²´í¬
                if semantic_score >= threshold:
                    # í‰ê°€ëœ ë…¼ë¬¸ì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                    evaluated_paper = paper.copy()
                    evaluated_paper['semantic_score'] = semantic_score
                    evaluated_paper['content_for_summary'] = paper_content
                    evaluated_papers.append(evaluated_paper)
                    
                    logger.debug(f"  {paper.get('arxiv_id')}: {semantic_score:.4f}")
            
            # ìœ ì‚¬ë„ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            evaluated_papers.sort(
                key=lambda x: x.get('semantic_score', 0),
                reverse=True
            )
            
            # top_k ì ìš©
            if top_k and len(evaluated_papers) > top_k:
                evaluated_papers = evaluated_papers[:top_k]
            
            logger.info(f"âœ“ í‰ê°€ ì™„ë£Œ: {len(evaluated_papers)}ê°œ ë…¼ë¬¸ ì„ ë³„ (ì„ê³„ê°’: {threshold})")
            
            return evaluated_papers
        
        except Exception as e:
            logger.error(f"ì˜ë¯¸ ê¸°ë°˜ í‰ê°€ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def vector_search(
        self,
        query: str,
        top_k: int = 5
    ) -> Tuple[List[Dict], List[float]]:
        """
        VectorStoreì—ì„œ ë²¡í„° ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        ì´ê²ƒì€ ChromaDBì˜ ë²¡í„° ê²€ìƒ‰ ê¸°ëŠ¥ì„ í™œìš©í•©ë‹ˆë‹¤.
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜
        
        Returns:
            (ê²€ìƒ‰ ê²°ê³¼ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸, ìœ ì‚¬ë„ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸)
        """
        
        try:
            logger.info(f"ë²¡í„° ê²€ìƒ‰: {query[:50]}... (top_k={top_k})")
            
            # ì¿¼ë¦¬ ì„ë² ë”©
            if embed_text is None:
                logger.error("embed_textë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return [], []
            
            query_embedding = embed_text(query)
            
            if hasattr(query_embedding, 'tolist'):
                query_embedding = query_embedding.tolist()
            
            # ChromaDB ê²€ìƒ‰
            results = self.vectorstore.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                include=["documents", "metadatas", "distances"]
            )
            
            # ê²°ê³¼ ì²˜ë¦¬
            papers = []
            similarities = []
            
            if results['ids'] and len(results['ids']) > 0:
                for i, doc_id in enumerate(results['ids'][0]):
                    metadata = results['metadatas'][0][i] if results['metadatas'] else {}
                    distance = results['distances'][0][i] if results['distances'] else 0
                    similarity = 1 - distance  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                    
                    paper = {
                        'arxiv_id': metadata.get('arxiv_id', ''),
                        'title': metadata.get('title', ''),
                        'abstract': results['documents'][0][i] if results['documents'] else '',
                        'authors': metadata.get('authors', '').split(', '),
                        'categories': metadata.get('categories', '').split(', '),
                        'published_date': metadata.get('published_date', ''),
                        'pdf_url': metadata.get('pdf_url', ''),
                        'html_url': metadata.get('html_url', ''),
                        'vector_similarity': similarity
                    }
                    
                    papers.append(paper)
                    similarities.append(similarity)
            
            logger.info(f"âœ“ ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(papers)}ê°œ ê²°ê³¼")
            
            return papers, similarities
        
        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return [], []


class WorkflowIntegration:
    """
    workflowì™€ VectorStoreì˜ í†µí•© ë ˆì´ì–´
    
    ì´ í´ë˜ìŠ¤ëŠ” workflowì˜ ê° ë…¸ë“œì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”
    í¸ì˜ ë©”ì„œë“œë“¤ì„ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def __init__(
        self,
        persist_directory: str = "./data/arxiv_vectorstore"
    ):
        """
        í†µí•© ë ˆì´ì–´ ì´ˆê¸°í™”
        
        Args:
            persist_directory: ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.vectorstore = ArxivPaperVectorStore(persist_directory)
        self.search_engine = SemanticSearchEngine(self.vectorstore)
        
        logger.info("âœ“ WorkflowIntegration ì´ˆê¸°í™” ì™„ë£Œ")
    
    def process_search_results_for_evaluation(
        self,
        arxiv_papers: List[Dict],
        original_query: str,
        num_papers_to_return: int = 3,
        similarity_threshold: float = 0.3
    ) -> Dict:
        """
        workflowì˜ evaluate_relevance_nodeì—ì„œ í˜¸ì¶œí•  ë©”ì¸ ë©”ì„œë“œ
        
        ì´ ë©”ì„œë“œëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
        1. arXiv APIì—ì„œ ê²€ìƒ‰ëœ ë…¼ë¬¸ë“¤ì„ VectorStoreì— ì €ì¥
        2. ê° ë…¼ë¬¸ì˜ ì˜ë¯¸ì  ê´€ë ¨ì„± í‰ê°€
        3. ê´€ë ¨ì„± ë†’ì€ ë…¼ë¬¸ë“¤ë§Œ ì„ ë³„í•˜ì—¬ ë°˜í™˜
        
        Args:
            arxiv_papers: arXiv APIì˜ ê²€ìƒ‰ ê²°ê³¼
            original_query: ì‚¬ìš©ìì˜ ì›ë˜ ì§ˆë¬¸
            num_papers_to_return: ë°˜í™˜í•  ë…¼ë¬¸ ê°œìˆ˜
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
        
        Returns:
            {
                'success': bool,
                'relevant_papers': List[Dict],  # ì˜ë¯¸ ê¸°ë°˜ìœ¼ë¡œ ì„ ë³„ëœ ë…¼ë¬¸
                'evaluation_details': Dict,      # í‰ê°€ ê²°ê³¼ ìƒì„¸ì •ë³´
                'message': str
            }
        """
        
        try:
            logger.info(f"[WORKFLOW_INTEGRATION] í‰ê°€ ì‹œì‘")
            logger.info(f"  - ê²€ìƒ‰ ê²°ê³¼: {len(arxiv_papers)}ê°œ")
            logger.info(f"  - ì›ë˜ ì¿¼ë¦¬: {original_query[:50]}...")
            logger.info(f"  - ì„ê³„ê°’: {similarity_threshold}")
            
            # 1ë‹¨ê³„: ë…¼ë¬¸ë“¤ì„ VectorStoreì— ì¶”ê°€
            add_result = self.vectorstore.add_papers_from_arxiv_search(arxiv_papers)
            
            if not add_result['success']:
                logger.warning(f"VectorStore ì¶”ê°€ ì‹¤íŒ¨: {add_result['message']}")
                # ê·¸ë˜ë„ ê³„ì† ì§„í–‰ (ì´ë¯¸ ì €ì¥ëœ ë…¼ë¬¸ë“¤ì´ ìˆì„ ìˆ˜ ìˆìŒ)
            
            # 2ë‹¨ê³„: ì˜ë¯¸ ê¸°ë°˜ í‰ê°€ ìˆ˜í–‰
            relevant_papers = self.search_engine.evaluate_papers_semantic_relevance(
                papers=arxiv_papers,
                query=original_query,
                top_k=num_papers_to_return,
                threshold=similarity_threshold
            )
            
            # 3ë‹¨ê³„: ê²°ê³¼ ì •ë¦¬
            logger.info(f"âœ“ í‰ê°€ ì™„ë£Œ: {len(relevant_papers)}ê°œ ë…¼ë¬¸ ì„ ë³„")
            
            return {
                "success": True,
                "relevant_papers": relevant_papers,
                "evaluation_details": {
                    "total_papers_evaluated": len(arxiv_papers),
                    "papers_passed_threshold": len(relevant_papers),
                    "threshold_used": similarity_threshold,
                    "num_papers_returned": min(len(relevant_papers), num_papers_to_return)
                },
                "message": f"{len(relevant_papers)}ê°œì˜ ê´€ë ¨ ë…¼ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤"
            }
        
        except Exception as e:
            logger.error(f"[WORKFLOW_INTEGRATION] í‰ê°€ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                "success": False,
                "relevant_papers": [],
                "evaluation_details": {},
                "message": f"ì˜¤ë¥˜: {str(e)}"
            }
    
    def get_statistics(self) -> Dict:
        """VectorStoreì˜ í†µê³„ ì •ë³´"""
        return {
            "total_papers": self.vectorstore.get_collection_count(),
            "collection_name": self.vectorstore.collection_name,
            "persist_directory": self.vectorstore.persist_directory
        }


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    print("\n" + "="*60)
    print("ğŸ”¬ VectorStoreì™€ Workflow í†µí•© ì˜ˆì‹œ")
    print("="*60 + "\n")
    
    # 1. í†µí•© ë ˆì´ì–´ ì´ˆê¸°í™”
    integration = WorkflowIntegration(
        persist_directory="./data/arxiv_vectorstore"
    )
    
    # 2. ìƒ˜í”Œ arXiv ë…¼ë¬¸ (ì‹¤ì œë¡œëŠ” search_papers_nodeì—ì„œ ì˜´)
    sample_papers = [
        {
            'arxiv_id': '2401.00001',
            'title': 'Attention Mechanisms in Neural Networks',
            'abstract': 'This paper explores attention mechanisms and their role in modern deep learning.',
            'authors': ['John Smith', 'Jane Doe'],
            'categories': ['cs.LG', 'cs.AI'],
            'published_date': '2024-01-15',
            'pdf_url': 'https://arxiv.org/pdf/2401.00001',
            'html_url': 'https://arxiv.org/abs/2401.00001'
        },
        {
            'arxiv_id': '2401.00002',
            'title': 'Efficient Transformers',
            'abstract': 'We propose an efficient transformer architecture for real-world applications.',
            'authors': ['Alice Chen'],
            'categories': ['cs.LG'],
            'published_date': '2024-01-18',
            'pdf_url': 'https://arxiv.org/pdf/2401.00002',
            'html_url': 'https://arxiv.org/abs/2401.00002'
        }
    ]
    
    # 3. ì˜ë¯¸ ê¸°ë°˜ í‰ê°€ ìˆ˜í–‰
    user_query = "attention mechanisms and efficiency in transformers"
    
    result = integration.process_search_results_for_evaluation(
        arxiv_papers=sample_papers,
        original_query=user_query,
        num_papers_to_return=2,
        similarity_threshold=0.3
    )
    
    print(f"í‰ê°€ ê²°ê³¼: {result['message']}")
    print(f"\nì„ ë³„ëœ ë…¼ë¬¸:")
    for i, paper in enumerate(result['relevant_papers'], 1):
        print(f"\n{i}. {paper['title']}")
        print(f"   ìœ ì‚¬ë„: {paper['semantic_score']:.4f}")
        print(f"   ì €ì: {', '.join(paper['authors'])}")
    
    # 4. í†µê³„
    stats = integration.get_statistics()
    print(f"\nğŸ“Š VectorStore í†µê³„: {stats}")