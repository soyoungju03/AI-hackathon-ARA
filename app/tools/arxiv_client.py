"""
arXiv API í´ë¼ì´ì–¸íŠ¸
arXivì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  ë©”íƒ€ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

arXiv API ë¬¸ì„œ: https://arxiv.org/help/api/user-manual
"""

import requests
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta
import time
from urllib.parse import quote
import xml.etree.ElementTree as ET


class ArxivClient:
    """arXiv APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë…¼ë¬¸ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” í´ë˜ìŠ¤"""
    
    BASE_URL = "http://export.arxiv.org/api/query"
    
    # arXiv ì¹´í…Œê³ ë¦¬ ë§¤í•‘
    CATEGORIES = {
        'cs.AI': 'ì¸ê³µì§€ëŠ¥',
        'cs.LG': 'ê¸°ê³„í•™ìŠµ',
        'cs.CL': 'ìì—°ì–´ ì²˜ë¦¬',
        'cs.CV': 'ì»´í“¨í„° ë¹„ì „',
        'cs.NE': 'ì‹ ê²½ë§',
        'stat.ML': 'í†µê³„ ë¨¸ì‹ ëŸ¬ë‹',
        'physics.data-an': 'ë°ì´í„° ë¶„ì„'
    }
    
    def __init__(self, max_results_per_query: int = 100, delay: float = 3):
        """
        ArxivClient ì´ˆê¸°í™”
        
        Args:
            max_results_per_query: í•œ ë²ˆì˜ ì¿¼ë¦¬ë¡œ ê°€ì ¸ì˜¬ ìµœëŒ€ ë…¼ë¬¸ ìˆ˜
            delay: API ìš”ì²­ ì‚¬ì´ì˜ ì§€ì—°ì‹œê°„ (ì´ˆ) - arXiv API ì •ì±… ì¤€ìˆ˜
        """
        self.max_results_per_query = max_results_per_query
        self.delay = delay
        self.session = requests.Session()
        self.last_request_time = 0
        
        print("âœ“ arXiv í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"  - ìµœëŒ€ ê²°ê³¼ ìˆ˜: {max_results_per_query}")
        print(f"  - ìš”ì²­ ì§€ì—°: {delay}ì´ˆ")
    
    def _rate_limit(self):
        """API ì†ë„ ì œí•œì„ ì§€í‚¤ê¸° ìœ„í•´ ëŒ€ê¸°"""
        elapsed = time.time() - self.last_request_time
        if elapsed < self.delay:
            time.sleep(self.delay - elapsed)
        self.last_request_time = time.time()
    
    def search_by_keyword(
        self,
        keywords: List[str],
        max_results: int = 50,
        sort_by: str = "submittedDate",
        sort_order: str = "descending",
        categories: Optional[List[str]] = None
    ) -> Tuple[List[Dict], int]:
        """
        í‚¤ì›Œë“œë¡œ arXiv ë…¼ë¬¸ ê²€ìƒ‰
        
        Args:
            keywords: ê²€ìƒ‰í•  í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            max_results: ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
            sort_by: ì •ë ¬ ê¸°ì¤€ ("submittedDate", "lastUpdatedDate", "relevance")
            sort_order: ì •ë ¬ ìˆœì„œ ("ascending", "descending")
            categories: ì œí•œí•  ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ['cs.LG', 'cs.AI'])
        
        Returns:
            (ê²€ìƒ‰ëœ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸, ì´ ê²°ê³¼ ìˆ˜)
        """
        
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        query_parts = []
        
        for keyword in keywords:
            # ê° í‚¤ì›Œë“œë¥¼ ì œëª©, ìš”ì•½, ì €ìì—ì„œ ê²€ìƒ‰
            query_parts.append(f"(ti:\"{keyword}\" OR abs:\"{keyword}\")")
        
        query = " AND ".join(query_parts) if query_parts else "*"
        
        # ì¹´í…Œê³ ë¦¬ í•„í„° ì¶”ê°€
        if categories:
            cat_query = " OR ".join([f"cat:{cat}" for cat in categories])
            query += f" AND ({cat_query})"
        
        return self._execute_query(
            query=query,
            max_results=max_results,
            sort_by=sort_by,
            sort_order=sort_order
        )
    
    def search_by_category(
        self,
        category: str,
        max_results: int = 50,
        days_back: int = 7,
        sort_by: str = "submittedDate"
    ) -> Tuple[List[Dict], int]:
        """
        ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìµœì‹  ë…¼ë¬¸ ê²€ìƒ‰
        
        Args:
            category: arXiv ì¹´í…Œê³ ë¦¬ (ì˜ˆ: 'cs.LG', 'cs.AI')
            max_results: ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
            days_back: ëª‡ ì¼ ì „ê¹Œì§€ì˜ ë…¼ë¬¸ì„ ê²€ìƒ‰í• ì§€
            sort_by: ì •ë ¬ ê¸°ì¤€
        
        Returns:
            (ê²€ìƒ‰ëœ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸, ì´ ê²°ê³¼ ìˆ˜)
        """
        
        # ìµœê·¼ Nì¼ ë…¼ë¬¸ë§Œ ê²€ìƒ‰
        start_date = datetime.utcnow() - timedelta(days=days_back)
        date_str = start_date.strftime("%Y%m%d%H%M%S")
        
        query = f"cat:{category} AND submittedDate:[{date_str} TO 9999999999]"
        
        return self._execute_query(
            query=query,
            max_results=max_results,
            sort_by=sort_by,
            sort_order="descending"
        )
    
    def search_by_arxiv_id(self, arxiv_id: str) -> Optional[Dict]:
        """
        íŠ¹ì • arXiv IDë¡œ ë…¼ë¬¸ ê²€ìƒ‰
        
        Args:
            arxiv_id: arXiv ë…¼ë¬¸ ID (ì˜ˆ: "2401.00001")
        
        Returns:
            ë…¼ë¬¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬, ë˜ëŠ” ì°¾ì§€ ëª»í•œ ê²½ìš° None
        """
        
        query = f"arxivID:{arxiv_id}"
        papers, _ = self._execute_query(query, max_results=1)
        
        return papers[0] if papers else None
    
    def _execute_query(
        self,
        query: str,
        max_results: int,
        sort_by: str = "submittedDate",
        sort_order: str = "descending"
    ) -> Tuple[List[Dict], int]:
        """
        arXiv APIì— ì¿¼ë¦¬ ì‹¤í–‰
        
        Args:
            query: arXiv ê²€ìƒ‰ ì¿¼ë¦¬
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            sort_by: ì •ë ¬ ê¸°ì¤€
            sort_order: ì •ë ¬ ìˆœì„œ
        
        Returns:
            (ë…¼ë¬¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸, ì´ ê²°ê³¼ ìˆ˜)
        """
        
        self._rate_limit()
        
        params = {
            'search_query': query,
            'start': 0,
            'max_results': min(max_results, self.max_results_per_query),
            'sortBy': sort_by,
            'sortOrder': sort_order
        }
        
        try:
            print(f"\nğŸ” arXiv ê²€ìƒ‰ ì¤‘...")
            print(f"   ì¿¼ë¦¬: {query[:100]}..." if len(query) > 100 else f"   ì¿¼ë¦¬: {query}")
            
            response = self.session.get(
                self.BASE_URL,
                params=params,
                timeout=10
            )
            response.raise_for_status()
            
            papers, total_results = self._parse_response(response.text)
            
            print(f"âœ“ ê²€ìƒ‰ ì™„ë£Œ: {len(papers)}ê°œ ë…¼ë¬¸ ë°œê²¬ (ì´ {total_results}ê°œ ì¤‘)")
            
            return papers, total_results
        
        except requests.exceptions.RequestException as e:
            print(f"âŒ API ìš”ì²­ ì‹¤íŒ¨: {str(e)}")
            return [], 0
    
    def _parse_response(self, xml_content: str) -> Tuple[List[Dict], int]:
        """
        arXiv APIì˜ XML ì‘ë‹µì„ íŒŒì‹±
        
        Args:
            xml_content: API ì‘ë‹µ XML
        
        Returns:
            (ë…¼ë¬¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸, ì´ ê²°ê³¼ ìˆ˜)
        """
        
        papers = []
        total_results = 0
        
        try:
            root = ET.fromstring(xml_content)
            
            # XML ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜
            namespaces = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            # ì „ì²´ ê²°ê³¼ ìˆ˜ íŒŒì‹±
            total_elem = root.find('atom:totalResults', namespaces)
            if total_elem is not None:
                total_results = int(total_elem.text)
            
            # ê° ë…¼ë¬¸ íŒŒì‹±
            for entry in root.findall('atom:entry', namespaces):
                try:
                    paper_info = self._extract_paper_info(entry, namespaces)
                    if paper_info:
                        papers.append(paper_info)
                except Exception as e:
                    print(f"âš ï¸ ë…¼ë¬¸ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                    continue
            
            return papers, total_results
        
        except ET.ParseError as e:
            print(f"âŒ XML íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            return [], 0
    
    def _extract_paper_info(self, entry, namespaces: Dict) -> Optional[Dict]:
        """
        XML ì—”íŠ¸ë¦¬ì—ì„œ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ
        
        Returns:
            ë…¼ë¬¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        
        atom_ns = namespaces['atom']
        arxiv_ns = namespaces['arxiv']
        
        # ê¸°ë³¸ ì •ë³´
        paper_id = entry.find(f'{{{arxiv_ns}}}id')
        title = entry.find(f'{{{atom_ns}}}title')
        summary = entry.find(f'{{{atom_ns}}}summary')
        published = entry.find(f'{{{atom_ns}}}published')
        updated = entry.find(f'{{{atom_ns}}}updated')
        
        if not all([paper_id, title, summary]):
            return None
        
        # arXiv ID ì •ì œ (ë²„ì „ ë²ˆí˜¸ ì œê±°)
        arxiv_id = paper_id.text.split('/abs/')[-1]
        
        # ì €ì ì¶”ì¶œ
        authors = []
        for author in entry.findall(f'{{{atom_ns}}}author'):
            name_elem = author.find(f'{{{atom_ns}}}name')
            if name_elem is not None:
                authors.append(name_elem.text)
        
        # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
        categories = []
        for category in entry.findall(f'{{{arxiv_ns}}}primary_category'):
            term = category.get('term')
            if term:
                categories.append(term)
        
        for category in entry.findall(f'{{{atom_ns}}}category'):
            term = category.get('term')
            if term:
                categories.append(term)
        
        categories = list(set(categories))  # ì¤‘ë³µ ì œê±°
        
        # ë§í¬ ì¶”ì¶œ
        pdf_url = ""
        html_url = ""
        for link in entry.findall(f'{{{atom_ns}}}link'):
            rel = link.get('rel')
            href = link.get('href')
            
            if rel == 'alternate':
                html_url = href
            elif link.get('type') == 'application/pdf':
                pdf_url = href + '.pdf'  # PDF ë§í¬ ì™„ì„±
        
        # í…ìŠ¤íŠ¸ ì •ì œ
        def clean_text(text):
            if text is None:
                return ""
            return ' '.join(text.split())
        
        title_text = clean_text(title.text)
        summary_text = clean_text(summary.text)
        
        # ë¬¸ì„œ ìƒì„± (ë²¡í„°ìŠ¤í† ì–´ì— ì¶”ê°€í•  í˜•ì‹)
        paper_info = {
            'id': f"arxiv_{arxiv_id.replace('.', '_').replace('/', '_')}",
            'content': f"{title_text}\n\n{summary_text}",
            'metadata': {
                'arxiv_id': arxiv_id,
                'title': title_text,
                'authors': authors,
                'published_date': published.text if published is not None else "",
                'updated_date': updated.text if updated is not None else "",
                'summary': summary_text,
                'categories': categories,
                'primary_category': categories[0] if categories else "unknown",
                'pdf_url': pdf_url,
                'html_url': html_url or f"https://arxiv.org/abs/{arxiv_id}"
            }
        }
        
        return paper_info
    
    def get_trending_papers(
        self,
        category: str = "cs.LG",
        days_back: int = 7,
        max_results: int = 20
    ) -> List[Dict]:
        """
        íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ìµœì‹  ë…¼ë¬¸ ì¡°íšŒ
        
        Args:
            category: arXiv ì¹´í…Œê³ ë¦¬
            days_back: ìµœê·¼ Nì¼ì˜ ë…¼ë¬¸
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
        
        Returns:
            ë…¼ë¬¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        
        papers, _ = self.search_by_category(
            category=category,
            max_results=max_results,
            days_back=days_back
        )
        
        return papers
    
    def search_multiple_queries(
        self,
        queries: List[Dict],
        consolidate: bool = True
    ) -> List[Dict]:
        """
        ì—¬ëŸ¬ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
        
        Args:
            queries: ê²€ìƒ‰ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸. ê° ì¿¼ë¦¬ëŠ”:
                {
                    'type': 'keyword' | 'category',
                    'keywords': [...],  # type='keyword'ì¼ ë•Œ
                    'category': 'cs.LG',  # type='category'ì¼ ë•Œ
                    'max_results': 20
                }
            consolidate: ì¤‘ë³µëœ ë…¼ë¬¸ ì œê±° ì—¬ë¶€
        
        Returns:
            ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ í†µí•©í•œ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
        """
        
        all_papers = []
        seen_arxiv_ids = set()
        
        for query in queries:
            query_type = query.get('type', 'keyword')
            max_results = query.get('max_results', 50)
            
            if query_type == 'keyword':
                papers, _ = self.search_by_keyword(
                    keywords=query.get('keywords', []),
                    max_results=max_results
                )
            elif query_type == 'category':
                papers, _ = self.search_by_category(
                    category=query.get('category', 'cs.LG'),
                    max_results=max_results
                )
            else:
                continue
            
            # ì¤‘ë³µ ì œê±°
            for paper in papers:
                arxiv_id = paper['metadata']['arxiv_id']
                
                if consolidate and arxiv_id in seen_arxiv_ids:
                    continue
                
                seen_arxiv_ids.add(arxiv_id)
                all_papers.append(paper)
        
        print(f"\nğŸ“Š í†µí•© ê²°ê³¼: {len(all_papers)}ê°œ ë…¼ë¬¸ (ì¤‘ë³µ ì œê±°ë¨)")
        
        return all_papers


# ì‚¬ìš© ì˜ˆì‹œ í•¨ìˆ˜
def example_usage():
    """arXiv í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš© ì˜ˆì‹œ"""
    
    print("\n" + "="*60)
    print("ğŸ”¬ arXiv API í´ë¼ì´ì–¸íŠ¸ ì˜ˆì‹œ")
    print("="*60 + "\n")
    
    # 1. í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client = ArxivClient(max_results_per_query=100, delay=2)
    
    # 2. í‚¤ì›Œë“œ ê²€ìƒ‰
    print("\n" + "-"*60)
    print("1ï¸âƒ£ í‚¤ì›Œë“œ ê²€ìƒ‰: 'attention mechanism' AND 'transformers'")
    print("-"*60)
    
    papers, total = client.search_by_keyword(
        keywords=['attention mechanism', 'transformers'],
        max_results=5
    )
    
    for i, paper in enumerate(papers, 1):
        print(f"\n{i}. {paper['metadata']['title']}")
        print(f"   arXiv ID: {paper['metadata']['arxiv_id']}")
        print(f"   ì €ì: {', '.join(paper['metadata']['authors'][:2])}")
        print(f"   ê²Œì‹œì¼: {paper['metadata']['published_date'][:10]}")
        print(f"   ì¹´í…Œê³ ë¦¬: {', '.join(paper['metadata']['categories'])}")
    
    # 3. ì¹´í…Œê³ ë¦¬ë³„ ìµœì‹  ë…¼ë¬¸
    print("\n" + "-"*60)
    print("2ï¸âƒ£ ì¹´í…Œê³ ë¦¬ë³„ ìµœì‹  ë…¼ë¬¸: cs.LG (ìµœê·¼ 7ì¼)")
    print("-"*60)
    
    trending_papers = client.get_trending_papers(
        category='cs.LG',
        days_back=7,
        max_results=3
    )
    
    for i, paper in enumerate(trending_papers, 1):
        print(f"\n{i}. {paper['metadata']['title']}")
        print(f"   arXiv ID: {paper['metadata']['arxiv_id']}")
        print(f"   ìš”ì•½: {paper['metadata']['summary'][:200]}...")
    
    # 4. íŠ¹ì • IDë¡œ ê²€ìƒ‰
    print("\n" + "-"*60)
    print("3ï¸âƒ£ íŠ¹ì • ë…¼ë¬¸ ê²€ìƒ‰")
    print("-"*60)
    
    specific_paper = client.search_by_arxiv_id("2401.00001")
    if specific_paper:
        print(f"âœ“ ë…¼ë¬¸ ë°œê²¬:")
        print(f"  ì œëª©: {specific_paper['metadata']['title']}")
        print(f"  ì €ì: {', '.join(specific_paper['metadata']['authors'])}")
    else:
        print("ë…¼ë¬¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
    
    # 5. ì—¬ëŸ¬ ì¿¼ë¦¬ ê²€ìƒ‰
    print("\n" + "-"*60)
    print("4ï¸âƒ£ ë³µìˆ˜ ì¿¼ë¦¬ ê²€ìƒ‰")
    print("-"*60)
    
    queries = [
        {
            'type': 'keyword',
            'keywords': ['vision transformer'],
            'max_results': 5
        },
        {
            'type': 'category',
            'category': 'cs.CV',
            'max_results': 5
        }
    ]
    
    consolidated_papers = client.search_multiple_queries(queries, consolidate=True)
    
    print(f"\nìµœì¢… ê²°ê³¼: {len(consolidated_papers)}ê°œ ë…¼ë¬¸")
    print("\nìƒìœ„ 3ê°œ ë…¼ë¬¸:")
    for i, paper in enumerate(consolidated_papers[:3], 1):
        print(f"{i}. {paper['metadata']['title']}")


if __name__ == "__main__":
    example_usage()