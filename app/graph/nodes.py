# -*- coding: utf-8 -*-
"""
ìˆ˜ì •ëœ LangGraph ë…¸ë“œë“¤ (PDF ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì™„ì „ í†µí•© + ë‹¨ìˆœí™” ë²„ì „)

ì£¼ìš” ìˆ˜ì •ì‚¬í•­:
1. analyze_question_nodeì—ì„œ ìë™ ìŠ¹ì¸ ë¡œì§ ì œê±°
2. ì¬ë¶„ì„ì´ë“  ì•„ë‹ˆë“  í•­ìƒ ì‚¬ìš©ìì—ê²Œ í‚¤ì›Œë“œ í™•ì¸ ìš”ì²­
3. ëª¨ë“  ë…¸ë“œì˜ ì™„ì „í•œ êµ¬í˜„ í¬í•¨ (search_papers, evaluate_relevance, summarize, generate_response)

ë°ì´í„° íë¦„:
ì‚¬ìš©ì ì§ˆë¬¸ â†’ ë¶„ì„ â†’ í‚¤ì›Œë“œ í™•ì¸ â†’ [ë‹¤ì‹œ ì„ íƒ ì‹œ ì¬ë¶„ì„ â†’ ë‹¤ì‹œ í™•ì¸] â†’
ë…¼ë¬¸ ìˆ˜ ì„ íƒ â†’ arXiv ê²€ìƒ‰ â†’ PDF ë‹¤ìš´ë¡œë“œ â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ â†’ ì²­í‚¹ â†’ ì„ë² ë”© â†’
ChromaDB ì €ì¥ â†’ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ â†’ ìš”ì•½ ë° ë‹µë³€
"""

import logging
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

from app.graph.state import (
    AgentState, 
    ReActStep, 
    InterruptData
)
from app.tools.paper_search.arxiv_tool import search_arxiv
from app.config import get_settings
from app.tools.pdf_embedding_pipeline_final import PDFEmbeddingPipeline
from app.tools.embeddings import SentenceTransformerEmbedding
from app.tools.vectorstore import ArxivPaperVectorStore

settings = get_settings()
logger = logging.getLogger(__name__)


# ============================================
# PDF ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (ì‹±ê¸€í†¤)
# ============================================

_pdf_pipeline = None

def get_pdf_pipeline():
    """PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    
    global _pdf_pipeline
    
    if _pdf_pipeline is None:
        try:
            logger.info("[INIT] PDF ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
            
            embedding_model = SentenceTransformerEmbedding(
                model_name="sentence-transformers/all-MiniLM-L6-v2"
            )
            
            vectorstore = ArxivPaperVectorStore(
                persist_directory="./data/arxiv_chunks",
                collection_name="arxiv_chunks"
            )
            
            _pdf_pipeline = PDFEmbeddingPipeline(
                embedding_model=embedding_model,
                vectorstore=vectorstore,
                chunk_chars=1800,
                overlap_chars=350,
                batch_size=32
            )
            
            logger.info("âœ“ PDF ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
            logger.info(f"  - ì„ë² ë”© ëª¨ë¸: all-MiniLM-L6-v2")
            logger.info(f"  - ë²¡í„° ì €ì¥ì†Œ: ChromaDB")
            logger.info(f"  - ì²­í¬ í¬ê¸°: 1800 ë¬¸ì (~450 í† í°)")
        
        except ImportError as e:
            logger.error(f"PDF íŒŒì´í”„ë¼ì¸ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {str(e)}")
            logger.error("pdf_embedding_pipeline_final.pyê°€ app/tools/ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”")
            raise
        except Exception as e:
            logger.error(f"PDF íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise
    
    return _pdf_pipeline


def get_llm(model: str = None):
    """LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return ChatOpenAI(
        model=model or settings.default_model,
        api_key=settings.openai_api_key,
        temperature=0.3
    )


# ============================================
# ë…¸ë“œ 1: ì§ˆë¬¸ ìˆ˜ì‹  (receive_question_node)
# ============================================

def receive_question_node(state: AgentState) -> dict:
    """ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ìˆ˜ì‹ í•˜ê³  ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
    
    user_question = state.get("user_question", "")
    
    logger.info("="*60)
    logger.info("[RECEIVE_QUESTION] ì‚¬ìš©ì ì§ˆë¬¸ ìˆ˜ì‹ ")
    logger.info("="*60)
    logger.info(f"ì§ˆë¬¸: {user_question}")
    
    thought_content = f'ì‚¬ìš©ì ì§ˆë¬¸ì„ ìˆ˜ì‹ í–ˆìŠµë‹ˆë‹¤: "{user_question}"\nì´ì œ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ í‚¤ì›Œë“œì™€ ì˜ë„ë¥¼ íŒŒì•…í•´ì•¼ í•©ë‹ˆë‹¤.'
    
    new_step = ReActStep(
        step_type="thought",
        content=thought_content
    )
    
    return {
        "react_steps": [new_step]
    }


# ============================================
# ë…¸ë“œ 2: ì§ˆë¬¸ ë¶„ì„ (analyze_question_node) - ğŸ”‘ ìˆ˜ì •ë¨
# ============================================

QUESTION_ANALYSIS_PROMPT = """
ë‹¹ì‹ ì€ í•™ìˆ  ì—°êµ¬ ì§ˆë¬¸ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

## ì‚¬ìš©ì ì§ˆë¬¸
{question}

## ë¶„ì„í•´ì•¼ í•  í•­ëª©

1. **í•µì‹¬ í‚¤ì›Œë“œ**: ë…¼ë¬¸ ê²€ìƒ‰ì— ì‚¬ìš©í•  í•µì‹¬ ê¸°ìˆ  í‚¤ì›Œë“œ 2-5ê°œ
   - ì˜ì–´ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”
   - êµ¬ì²´ì ì´ê³  ê²€ìƒ‰ íš¨ê³¼ê°€ ì¢‹ì€ í‚¤ì›Œë“œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”

2. **ì§ˆë¬¸ ì˜ë„**: ì‚¬ìš©ìê°€ ì•Œê³  ì‹¶ì–´í•˜ëŠ” ê²ƒì´ ë¬´ì—‡ì¸ì§€
   - "ìµœì‹  ì—°êµ¬ ë™í–¥" / "íŠ¹ì • ê¸°ìˆ  ì„¤ëª…" / "ë¹„êµ ë¶„ì„" / "ì‘ìš© ì‚¬ë¡€" ë“±

3. **ì—°êµ¬ ë„ë©”ì¸**: ì–´ë–¤ í•™ë¬¸ ë¶„ì•¼ì— í•´ë‹¹í•˜ëŠ”ì§€
   - "computer science" / "physics" / "mathematics" / "biology" ë“±

## ì‘ë‹µ í˜•ì‹ (ë°˜ë“œì‹œ ì´ í˜•ì‹ì„ ë”°ë¼ì£¼ì„¸ìš”)
KEYWORDS: keyword1, keyword2, keyword3
INTENT: ì§ˆë¬¸ ì˜ë„ ì„¤ëª…
DOMAIN: ì—°êµ¬ ë„ë©”ì¸
"""


def analyze_question_node(state: AgentState) -> dict:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    ğŸ”‘ í•µì‹¬ ìˆ˜ì •: ì¬ë¶„ì„ì´ë“  ì•„ë‹ˆë“  í•­ìƒ ì‚¬ìš©ìì—ê²Œ í™•ì¸ ìš”ì²­
    ìë™ ìŠ¹ì¸ ë¡œì§ ì œê±°ë¨
    """
    
    user_question = state.get("user_question", "")
    is_reanalyzing = state.get("is_reanalyzing", False)
    
    logger.info("="*60)
    logger.info("[ANALYZE_QUESTION] ì§ˆë¬¸ ë¶„ì„ ì‹œì‘")
    logger.info(f"  ì¬ë¶„ì„ ëª¨ë“œ: {is_reanalyzing}")
    logger.info("="*60)
    logger.info(f"ë¶„ì„ ëŒ€ìƒ: {user_question[:50]}...")
    
    try:
        llm = get_llm(settings.light_model)
        prompt = QUESTION_ANALYSIS_PROMPT.format(question=user_question)
        
        logger.info("LLMì— ì§ˆë¬¸ ë¶„ì„ ìš”ì²­ ì „ì†¡...")
        
        response = llm.invoke([
            SystemMessage(content="ë‹¹ì‹ ì€ í•™ìˆ  ì—°êµ¬ ì§ˆë¬¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
            HumanMessage(content=prompt)
        ])
        
        logger.info("âœ“ LLM ì‘ë‹µ ìˆ˜ì‹ ")
        
        # LLM ì‘ë‹µ íŒŒì‹±
        response_text = response.content
        keywords = []
        intent = ""
        domain = ""
        
        for line in response_text.strip().split('\n'):
            line = line.strip()
            if line.startswith("KEYWORDS:"):
                keywords_str = line.replace("KEYWORDS:", "").strip()
                keywords = [k.strip() for k in keywords_str.split(",") if k.strip()]
            elif line.startswith("INTENT:"):
                intent = line.replace("INTENT:", "").strip()
            elif line.startswith("DOMAIN:"):
                domain = line.replace("DOMAIN:", "").strip()
        
        logger.info(f"âœ“ ë¶„ì„ ì™„ë£Œ")
        logger.info(f"  ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
        logger.info(f"  ì§ˆë¬¸ ì˜ë„: {intent}")
        logger.info(f"  ì—°êµ¬ ë„ë©”ì¸: {domain}")
        
        # ì¬ë¶„ì„ ëª¨ë“œì— ë”°ë¼ ë‹¤ë¥¸ ë©”ì‹œì§€ í‘œì‹œ
        if is_reanalyzing:
            observation_content = f"""ì§ˆë¬¸ ì¬ë¶„ì„ ì™„ë£Œ:
- ìƒˆë¡œìš´ í‚¤ì›Œë“œ: {', '.join(keywords)}
- ì§ˆë¬¸ ì˜ë„: {intent}
- ì—°êµ¬ ë„ë©”ì¸: {domain}

ìƒˆë¡œìš´ í‚¤ì›Œë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."""
            logger.info("  â†’ ì¬ë¶„ì„ ì™„ë£Œ: ì‚¬ìš©ì í™•ì¸ ëŒ€ê¸°")
        else:
            observation_content = f"""ì§ˆë¬¸ ë¶„ì„ ì™„ë£Œ:
- ì¶”ì¶œëœ í‚¤ì›Œë“œ: {', '.join(keywords)}
- ì§ˆë¬¸ ì˜ë„: {intent}
- ì—°êµ¬ ë„ë©”ì¸: {domain}"""
        
        new_step = ReActStep(
            step_type="observation",
            content=observation_content
        )
        
        # ğŸ”‘ í•µì‹¬ ìˆ˜ì •: ìë™ ìŠ¹ì¸ ì œê±°, í•­ìƒ Noneìœ¼ë¡œ ì„¤ì •
        # ì´ì „: "keyword_confirmation_response": "confirmed" if is_reanalyzing else None
        # ì´í›„: í•­ìƒ None (í•­ìƒ ì‚¬ìš©ì í™•ì¸ í•„ìš”)
        return {
            "extracted_keywords": keywords,
            "question_intent": intent,
            "question_domain": domain,
            "is_reanalyzing": False,  # í”Œë˜ê·¸ ì´ˆê¸°í™”
            "keyword_confirmation_response": None,  # ğŸ”‘ í•­ìƒ None
            "react_steps": [new_step]
        }
        
    except Exception as e:
        logger.error(f"ì§ˆë¬¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return {
            "extracted_keywords": ["research"],
            "question_intent": "general research",
            "question_domain": "computer science",
            "is_reanalyzing": False,
            "keyword_confirmation_response": None,
            "error_message": str(e),
            "react_steps": [ReActStep(step_type="observation", content=f"ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {str(e)}")]
        }


# ============================================
# ë…¸ë“œ 3: í‚¤ì›Œë“œ í™•ì¸ ìš”ì²­ (request_keyword_confirmation_node)
# ì²« ë²ˆì§¸ Human-in-the-Loop Interrupt ì§€ì 
# ============================================

def request_keyword_confirmation_node(state: AgentState) -> dict:
    """
    ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ë§ëŠ”ì§€ ì‚¬ìš©ìì—ê²Œ í™•ì¸ë°›ìŠµë‹ˆë‹¤.
    
    ì²« ë²ˆì§¸ Human-in-the-Loop Interrupt ì§€ì ì…ë‹ˆë‹¤.
    ì›Œí¬í”Œë¡œìš°ëŠ” ì—¬ê¸°ì„œ ë©ˆì¶”ê³  ì‚¬ìš©ìê°€ ì‘ë‹µí•  ë•Œê¹Œì§€ ëŒ€ê¸°í•©ë‹ˆë‹¤.
    """
    
    keywords = state.get("extracted_keywords", [])
    
    logger.info("[REQUEST_KEYWORD_CONFIRMATION] ì‚¬ìš©ì í™•ì¸ ëŒ€ê¸° ì‹œì‘")
    
    message = f"""
ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.

í‚¤ì›Œë“œ: {', '.join(keywords) if keywords else 'ì—†ìŒ'}

ë§ìœ¼ë©´ "í™•ì¸"ì„, ìˆ˜ì •ì´ í•„ìš”í•˜ë©´ "ë‹¤ì‹œ"ë¼ê³  ì…ë ¥í•´ì£¼ì„¸ìš”.
    """.strip()
    
    interrupt_data = InterruptData(
        interrupt_type="confirm_keywords",
        message=message,
        options=["í™•ì¸", "ë‹¤ì‹œ"],
        default_value="í™•ì¸",
        metadata={
            "keywords": keywords,
            "stage": 1
        }
    )
    
    thought_content = "í‚¤ì›Œë“œ ì¶”ì¶œì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ í™•ì¸ì„ ìš”ì²­í•©ë‹ˆë‹¤."
    
    new_step = ReActStep(
        step_type="thought",
        content=thought_content
    )
    
    return {
        "interrupt_data": interrupt_data,
        "waiting_for": "keyword_confirmation",
        "interrupt_stage": 1,
        "waiting_for_user": True,
        "react_steps": [new_step]
    }


# ============================================
# ë…¸ë“œ 4: í‚¤ì›Œë“œ í™•ì¸ ì‘ë‹µ ì²˜ë¦¬
# ============================================

def process_keyword_confirmation_response_node(state: AgentState) -> dict:
    """
    ì‚¬ìš©ìì˜ í‚¤ì›Œë“œ í™•ì¸ ì‘ë‹µì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    - "ë‹¤ì‹œ" â†’ is_reanalyzing=True ì„¤ì •, ì§ˆë¬¸ ë¶„ì„ ë‹¨ê³„ë¡œ ëŒì•„ê°
    - ê·¸ ì™¸ ("í™•ì¸" ë“±) â†’ is_reanalyzing=False, ë…¼ë¬¸ ìˆ˜ ì„ íƒ ë‹¨ê³„ë¡œ ì§„í–‰
    """
    
    user_response = state.get("user_response", "").strip().lower()
    
    logger.info("[PROCESS_KEYWORD_CONFIRMATION] ì‚¬ìš©ì ì‘ë‹µ ì²˜ë¦¬")
    logger.info(f"  ì‘ë‹µ: {user_response}")
    
    # "ë‹¤ì‹œ" ì‘ë‹µ í™•ì¸
    if user_response in ["ë‹¤ì‹œ", "retry", "ë‹¤ì‹œí•˜ê¸°", "ìˆ˜ì •", "ë‹¤ì‹œí•´", "reanalyze"]:
        logger.info("  â†’ 'ë‹¤ì‹œ' ì„ íƒ: ì¬ë¶„ì„ ëª¨ë“œ í™œì„±í™”")
        
        observation_content = "ì‚¬ìš©ìê°€ í‚¤ì›Œë“œ ì¬ë¶„ì„ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ ë¶„ì„í•©ë‹ˆë‹¤."
        
        new_step = ReActStep(
            step_type="observation",
            content=observation_content
        )
        
        return {
            "keyword_confirmation_response": "retry",
            "is_reanalyzing": True,  # í•µì‹¬: ì¬ë¶„ì„ ëª¨ë“œ í™œì„±í™”
            "waiting_for": None,
            "waiting_for_user": False,
            "interrupt_data": None,
            "react_steps": [new_step],
            "user_response": None
        }
    
    # ê·¸ ì™¸ì˜ ê²½ìš° "í™•ì¸"ìœ¼ë¡œ ì²˜ë¦¬
    logger.info("  â†’ 'í™•ì¸' ì„ íƒ: ë…¼ë¬¸ ìˆ˜ ì„ íƒ ë‹¨ê³„ë¡œ ì´ë™")
    
    observation_content = f"ì‚¬ìš©ìê°€ í‚¤ì›Œë“œë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. í‚¤ì›Œë“œ: {', '.join(state.get('extracted_keywords', []))}"
    
    new_step = ReActStep(
        step_type="observation",
        content=observation_content
    )
    
    return {
        "keyword_confirmation_response": "confirmed",
        "is_reanalyzing": False,
        "waiting_for": None,
        "waiting_for_user": False,
        "interrupt_data": None,
        "interrupt_stage": 1,
        "react_steps": [new_step],
        "user_response": None
    }


# ============================================
# ë…¸ë“œ 5: ë…¼ë¬¸ ìˆ˜ ì„ íƒ ìš”ì²­ (request_paper_count_node)
# ë‘ ë²ˆì§¸ Human-in-the-Loop Interrupt ì§€ì 
# ============================================

def request_paper_count_node(state: AgentState) -> dict:
    """
    ëª‡ ê°œì˜ ë…¼ë¬¸ì„ ê²€ìƒ‰í• ì§€ ì‚¬ìš©ìì—ê²Œ ì„ íƒë°›ìŠµë‹ˆë‹¤.
    
    ë‘ ë²ˆì§¸ Human-in-the-Loop Interrupt ì§€ì ì…ë‹ˆë‹¤.
    ì›Œí¬í”Œë¡œìš°ëŠ” ì—¬ê¸°ì„œ ë©ˆì¶”ê³  ì‚¬ìš©ìì˜ ì„ íƒì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.
    """
    
    logger.info("[REQUEST_PAPER_COUNT] ì‚¬ìš©ì ì„ íƒ ëŒ€ê¸° ì‹œì‘")
    
    message = """
ê²€ìƒ‰í•  ë…¼ë¬¸ì˜ ê°œìˆ˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.

1ë¶€í„° 10 ì‚¬ì´ì˜ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.
(ê¸°ë³¸ê°’: 3ê°œ)

ë” ë§ì€ ë…¼ë¬¸ì„ ì„ íƒí• ìˆ˜ë¡ ì²˜ë¦¬ ì‹œê°„ì´ ê¸¸ì–´ì§‘ë‹ˆë‹¤.
    """.strip()
    
    interrupt_data = InterruptData(
        interrupt_type="select_paper_count",
        message=message,
        options=["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"],
        default_value="3",
        metadata={
            "stage": 2
        }
    )
    
    thought_content = "í‚¤ì›Œë“œ í™•ì¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ê²€ìƒ‰í•  ë…¼ë¬¸ ìˆ˜ë¥¼ ì‚¬ìš©ìì—ê²Œ ì„ íƒë°›ìŠµë‹ˆë‹¤."
    
    new_step = ReActStep(
        step_type="thought",
        content=thought_content
    )
    
    return {
        "interrupt_data": interrupt_data,
        "waiting_for": "paper_count_selection",
        "interrupt_stage": 2,
        "waiting_for_user": True,
        "react_steps": [new_step]
    }


# ============================================
# ë…¸ë“œ 6: ë…¼ë¬¸ ìˆ˜ ì‘ë‹µ ì²˜ë¦¬
# ============================================

def process_paper_count_response_node(state: AgentState) -> dict:
    """
    ì‚¬ìš©ìê°€ ì„ íƒí•œ ë…¼ë¬¸ ìˆ˜ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    ì…ë ¥ê°’ì„ ì •ìˆ˜ë¡œ íŒŒì‹±í•˜ê³ , 1-10 ë²”ìœ„ë¡œ ì œí•œí•©ë‹ˆë‹¤.
    ì˜ëª»ëœ ì…ë ¥ì˜ ê²½ìš° ê¸°ë³¸ê°’ 3ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    
    user_response = state.get("user_response", "3")
    
    logger.info("[PROCESS_PAPER_COUNT] ì‚¬ìš©ì ì‘ë‹µ ì²˜ë¦¬")
    logger.info(f"  ì‘ë‹µ: {user_response}")
    
    try:
        paper_count = int(user_response)
        # ìœ íš¨í•œ ë²”ìœ„ë¡œ ì œí•œ
        paper_count = max(1, min(10, paper_count))
        logger.info(f"  â†’ í•´ì„ë¨: {paper_count}ê°œ")
    except ValueError:
        logger.warning(f"  â†’ ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥, ê¸°ë³¸ê°’ 3 ì‚¬ìš©")
        paper_count = 3
    
    observation_content = f"ì‚¬ìš©ìê°€ ë…¼ë¬¸ ìˆ˜ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤: {paper_count}ê°œ"
    
    new_step = ReActStep(
        step_type="observation",
        content=observation_content
    )
    
    return {
        "paper_count": paper_count,
        "waiting_for": None,
        "waiting_for_user": False,
        "interrupt_data": None,
        "interrupt_stage": 2,
        "react_steps": [new_step],
        "user_response": None
    }


# ============================================
# ë…¸ë“œ 7: ë…¼ë¬¸ ê²€ìƒ‰ + PDF ì²˜ë¦¬ (search_papers_node)
# ============================================

def search_papers_node(state: AgentState) -> dict:
    """
    arXivì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•œ í›„ PDF ì„ë² ë”© íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    ì‹¤í–‰ ë‹¨ê³„:
    1. arXiv APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë…¼ë¬¸ ê²€ìƒ‰
    2. ê° ë…¼ë¬¸ì˜ PDFë¥¼ ë‹¤ìš´ë¡œë“œ
    3. PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    4. í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (ì•½ 450 í† í°ì”©)
    5. ê° ì²­í¬ë¥¼ Sentence Transformersë¡œ ì„ë² ë”©
    6. ì„ë² ë”©ëœ ì²­í¬ë¥¼ ChromaDBì— ì €ì¥
    """
    
    keywords = state.get("extracted_keywords", [])
    paper_count = state.get("paper_count", 3)
    domain = state.get("question_domain", None)
    
    logger.info("="*60)
    logger.info("[SEARCH_PAPERS] ë…¼ë¬¸ ê²€ìƒ‰ + PDF ì²˜ë¦¬ ì‹œì‘")
    logger.info("="*60)
    logger.info(f"í‚¤ì›Œë“œ: {keywords}")
    logger.info(f"ê²€ìƒ‰ ê°œìˆ˜: {paper_count}ê°œ")
    logger.info(f"ë„ë©”ì¸: {domain or 'ì „ì²´'}")
    
    action_content = f"""ë…¼ë¬¸ ê²€ìƒ‰ ë° PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘:
- í‚¤ì›Œë“œ: {', '.join(keywords)}
- ê²€ìƒ‰ ê°œìˆ˜: {paper_count}ê°œ
- ë„ë©”ì¸: {domain or 'ì „ì²´'}

ì²˜ë¦¬ ë‹¨ê³„:
1) arXivì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰
2) ê° ë…¼ë¬¸ì˜ PDF ë‹¤ìš´ë¡œë“œ
3) PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
4) í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
5) ê° ì²­í¬ë¥¼ ì„ë² ë”©
6) ChromaDBì— ì €ì¥"""
    
    action_step = ReActStep(
        step_type="action",
        content=action_content
    )
    
    try:
        # Step 1: arXivì—ì„œ ë…¼ë¬¸ ê²€ìƒ‰
        logger.info("\nStep 1: arXiv ê²€ìƒ‰ ì‹¤í–‰...")
        
        papers = search_arxiv(
            keywords=keywords,
            max_results=paper_count,
            domain=domain
        )
        
        if not papers:
            logger.warning("ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            observation_step = ReActStep(
                step_type="observation",
                content="arXivì—ì„œ ê´€ë ¨ ë…¼ë¬¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ì£¼ì„¸ìš”."
            )
            
            return {
                "papers": [],
                "chunks_saved": 0,
                "react_steps": [action_step, observation_step],
                "error_message": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
            }
        
        logger.info(f"âœ“ {len(papers)}ê°œ ë…¼ë¬¸ ê²€ìƒ‰ ì™„ë£Œ")
        
        # Step 2: PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        logger.info("\nStep 2: PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”...")
        
        pipeline = get_pdf_pipeline()
        
        logger.info(f"âœ“ íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ")
        
        # Step 3: ë…¼ë¬¸ì„ íŒŒì´í”„ë¼ì¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        logger.info("\nStep 3: ë…¼ë¬¸ ë°ì´í„° ë³€í™˜...")
        
        papers_for_pipeline = []
        for paper in papers:
            arxiv_id = paper.url.split('/')[-1] if hasattr(paper, 'url') and paper.url else 'unknown'
            
            paper_dict = {
                'arxiv_id': arxiv_id,
                'title': getattr(paper, 'title', ''),
                'abstract': getattr(paper, 'abstract', ''),
                'authors': getattr(paper, 'authors', []),
                'published_date': getattr(paper, 'published_date', ''),
                'categories': getattr(paper, 'categories', []),
                'url': getattr(paper, 'url', ''),
            }
            papers_for_pipeline.append(paper_dict)
        
        logger.info(f"âœ“ {len(papers_for_pipeline)}ê°œ ë…¼ë¬¸ ë³€í™˜ ì™„ë£Œ")
        
        # Step 4: PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë°°ì¹˜ ëª¨ë“œ)
        logger.info("\nStep 4: PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...")
        logger.info("  (ì´ ë‹¨ê³„ëŠ” ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        
        batch_result = pipeline.process_papers_batch(
            papers=papers_for_pipeline,
            max_pages=10
        )
        
        logger.info(f"âœ“ PDF ì²˜ë¦¬ ì™„ë£Œ")
        
        # ì²˜ë¦¬ ê²°ê³¼ ë¶„ì„
        total_chunks_created = sum(r.get('chunks_created', 0) for r in batch_result['results'])
        total_chunks_saved = batch_result['total_chunks']
        
        logger.info(f"\nì²˜ë¦¬ ê²°ê³¼ í†µê³„:")
        logger.info(f"  - ì²˜ë¦¬ëœ ë…¼ë¬¸: {batch_result['successful']}/{len(papers)}")
        logger.info(f"  - ìƒì„±ëœ ì²­í¬: {total_chunks_created}ê°œ")
        logger.info(f"  - ì €ì¥ëœ ì²­í¬: {total_chunks_saved}ê°œ")
        logger.info(f"  - ì²˜ë¦¬ ì‹œê°„: {batch_result['time']:.1f}ì´ˆ")
        
        observation_content = f"""ê²€ìƒ‰ ë° PDF ì²˜ë¦¬ ì™„ë£Œ:

ê²€ìƒ‰ ê²°ê³¼:
- ê²€ìƒ‰ëœ ë…¼ë¬¸: {len(papers)}ê°œ
- ì²˜ë¦¬ ì„±ê³µ: {batch_result['successful']}ê°œ
- ì²˜ë¦¬ ì‹¤íŒ¨: {batch_result['failed']}ê°œ

ì²­í¬ ì²˜ë¦¬ í†µê³„:
- ìƒì„±ëœ ì²­í¬: {total_chunks_created}ê°œ
- ì €ì¥ëœ ì²­í¬: {total_chunks_saved}ê°œ
- ì²˜ë¦¬ ì‹œê°„: {batch_result['time']:.1f}ì´ˆ

ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ 
ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        
        observation_step = ReActStep(
            step_type="observation",
            content=observation_content
        )
        
        return {
            "papers": papers,
            "chunks_saved": total_chunks_saved,
            "pdf_processing_result": batch_result,
            "react_steps": [action_step, observation_step],
            "error_message": None
        }
    
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        
        error_observation = ReActStep(
            step_type="observation",
            content=f"""ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:
{str(e)}

ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:
1. ì¸í„°ë„· ì—°ê²° ìƒíƒœ
2. arXiv ì„œë²„ ìƒíƒœ
3. ë””ìŠ¤í¬ ê³µê°„
4. ë©”ëª¨ë¦¬ ìš©ëŸ‰"""
        )
        
        return {
            "papers": [],
            "chunks_saved": 0,
            "react_steps": [action_step, error_observation],
            "error_message": str(e)
        }


# ============================================
# ë…¸ë“œ 8: ì˜ë¯¸ ê¸°ë°˜ ê´€ë ¨ì„± í‰ê°€ (evaluate_relevance_node)
# ============================================

def evaluate_relevance_node(state: AgentState) -> dict:
    """
    ChromaDBì— ì €ì¥ëœ ì²­í¬ë“¤ ì¤‘ì—ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼
    ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ë“¤ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ í†µí•´ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì²­í¬ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    
    user_question = state.get("user_question", "")
    paper_count = state.get("paper_count", 3)
    chunks_saved = state.get("chunks_saved", 0)
    
    logger.info("="*60)
    logger.info("[EVALUATE_RELEVANCE] ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì‹œì‘")
    logger.info("="*60)
    logger.info(f"ì§ˆë¬¸: {user_question[:50]}...")
    logger.info(f"ê²€ìƒ‰í•  ì²­í¬ ìˆ˜: {paper_count * 3}ê°œ")
    logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì²­í¬: {chunks_saved}ê°œ")
    
    action_content = f"""ì˜ë¯¸ ê¸°ë°˜ ì²­í¬ ê²€ìƒ‰:
- ì§ˆë¬¸: "{user_question[:60]}..."
- ê²€ìƒ‰ ë°©ì‹: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (Sentence Transformers)
- ëª©í‘œ ì²­í¬ ìˆ˜: {paper_count * 3}ê°œ
- ì‚¬ìš© ê°€ëŠ¥í•œ ì²­í¬: {chunks_saved}ê°œ"""
    
    action_step = ReActStep(
        step_type="action",
        content=action_content
    )
    
    if chunks_saved == 0:
        logger.warning("ì €ì¥ëœ ì²­í¬ê°€ ì—†ìŒ")
        observation_step = ReActStep(
            step_type="observation",
            content="""ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì‹¤íŒ¨:
ì´ì „ ë‹¨ê³„ì—ì„œ ì²­í¬ê°€ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
ê²€ìƒ‰ì´ ì‹¤íŒ¨í–ˆê±°ë‚˜ ëª¨ë“  PDF ì²˜ë¦¬ê°€ ì‹¤íŒ¨í–ˆì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤."""
        )
        
        return {
            "relevant_chunks": [],
            "evaluation_result": {
                "success": False,
                "message": "ì €ì¥ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤"
            },
            "react_steps": [action_step, observation_step]
        }
    
    try:
        logger.info("\nPDF íŒŒì´í”„ë¼ì¸ì—ì„œ ë²¡í„°ìŠ¤í† ì–´ ì ‘ê·¼...")
        
        pipeline = get_pdf_pipeline()
        vectorstore = pipeline.vectorstore
        embedding_model = pipeline.embedding_model
        
        logger.info("âœ“ ë²¡í„°ìŠ¤í† ì–´ ì ‘ê·¼ ì„±ê³µ")
        
        # Step 1: ì‚¬ìš©ì ì§ˆë¬¸ì„ ì„ë² ë”©
        logger.info("\nStep 1: ì§ˆë¬¸ ì„ë² ë”© ìƒì„±...")
        
        query_embedding = embedding_model.embed(user_question)
        
        logger.info(f"âœ“ ì„ë² ë”© ì™„ë£Œ (ì°¨ì›: {len(query_embedding)})")
        
        # Step 2: ChromaDBì—ì„œ ê²€ìƒ‰
        logger.info(f"\nStep 2: ChromaDBì—ì„œ ê²€ìƒ‰ (ìƒìœ„ {paper_count * 3}ê°œ)...")
        
        search_results = vectorstore.collection.query(
            query_embeddings=[query_embedding],
            n_results=paper_count * 3,
            include=["documents", "metadatas", "distances"]
        )
        
        logger.info(f"âœ“ ê²€ìƒ‰ ì™„ë£Œ: {len(search_results['ids'][0]) if search_results['ids'] else 0}ê°œ ê²°ê³¼")
        
        # Step 3: ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
        logger.info("\nStep 3: ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬...")
        
        relevant_chunks = []
        
        if search_results['ids'] and len(search_results['ids']) > 0:
            for i, chunk_id in enumerate(search_results['ids'][0]):
                distance = search_results['distances'][0][i]
                similarity_score = 1 - distance
                
                metadata = search_results['metadatas'][0][i] if search_results['metadatas'] else {}
                chunk_content = search_results['documents'][0][i] if search_results['documents'] else ''
                
                chunk_info = {
                    'chunk_id': chunk_id,
                    'content': chunk_content,
                    'similarity_score': float(similarity_score),
                    'arxiv_id': metadata.get('arxiv_id', ''),
                    'title': metadata.get('title', ''),
                    'section': metadata.get('section', ''),
                    'page_number': int(metadata.get('page_number', 1)) if metadata.get('page_number') else 1,
                    'chunk_index': metadata.get('chunk_index', ''),
                    'authors': metadata.get('authors', ''),
                    'metadata': metadata
                }
                
                relevant_chunks.append(chunk_info)
                
                if i < 3:
                    logger.debug(f"  ì²­í¬ {i+1}: ìœ ì‚¬ë„ {similarity_score:.4f}")
        
        logger.info(f"âœ“ {len(relevant_chunks)}ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ")
        
        observation_parts = [f"ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì™„ë£Œ: {len(relevant_chunks)}ê°œ ì²­í¬ ë°œê²¬"]
        observation_parts.append("\nê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ìƒìœ„ 3ê°œ ì²­í¬:")
        
        for i, chunk in enumerate(relevant_chunks[:3], 1):
            observation_parts.append(f"\n{i}. ìœ ì‚¬ë„: {chunk['similarity_score']:.4f}")
            observation_parts.append(f"   ë…¼ë¬¸: {chunk['title'][:40] if chunk['title'] else 'N/A'}...")
            if chunk.get('section'):
                observation_parts.append(f"   ì„¹ì…˜: {chunk['section']}")
            observation_parts.append(f"   ë‚´ìš©: {chunk['content'][:60]}...")
        
        observation_content = "\n".join(observation_parts)
        
        observation_step = ReActStep(
            step_type="observation",
            content=observation_content
        )
        
        logger.info(f"\nâœ“ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì™„ë£Œ")
        
        return {
            "relevant_chunks": relevant_chunks[:paper_count * 2],
            "evaluation_result": {
                "success": True,
                "message": f"{len(relevant_chunks)}ê°œì˜ ê´€ë ¨ ì²­í¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤"
            },
            "react_steps": [action_step, observation_step]
        }
    
    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        
        error_observation = ReActStep(
            step_type="observation",
            content=f"""ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜:
{str(e)}

ì›ì¸ ë¶„ì„:
- ChromaDB ì—°ê²° ì‹¤íŒ¨
- ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨
- ì§ˆë¬¸ ë²¡í„°í™” ì‹¤íŒ¨"""
        )
        
        return {
            "relevant_chunks": [],
            "evaluation_result": {
                "success": False,
                "message": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"
            },
            "react_steps": [action_step, error_observation]
        }


# ============================================
# ë…¸ë“œ 9: ë…¼ë¬¸ ìš”ì•½ ìƒì„± (summarize_papers_node)
# ============================================

SUMMARIZE_PROMPT = """
ë‹¤ìŒ ë…¼ë¬¸ì˜ ì´ˆë¡ì„ ì½ê³  í•µì‹¬ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

## ë…¼ë¬¸ ì œëª©
{title}

## ì´ˆë¡
{abstract}

## ìš”ì•½ í˜•ì‹

### í•µì‹¬ ì•„ì´ë””ì–´
ë…¼ë¬¸ì˜ ì£¼ìš” ê¸°ì—¬ì ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

### ì—°êµ¬ ë°°ê²½ ë° ë¬¸ì œì 
í•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.

### ì œì•ˆ ë°©ë²•ë¡ 
ë¬¸ì œ í•´ê²° ì ‘ê·¼ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.

### ì£¼ìš” ì„±ê³¼
ì‹¤í—˜ ê²°ê³¼ë‚˜ ë‹¬ì„±í•œ ì„±ê³¼ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.
"""


def summarize_papers_node(state: AgentState) -> dict:
    """
    ê´€ë ¨ ì²­í¬ë“¤ì´ ì†í•œ ë…¼ë¬¸ë“¤ì˜ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    relevant_chunks = state.get("relevant_chunks", [])
    papers = state.get("papers", [])
    
    logger.info("="*60)
    logger.info("[SUMMARIZE_PAPERS] ë…¼ë¬¸ ìš”ì•½ ìƒì„±")
    logger.info("="*60)
    logger.info(f"ê´€ë ¨ ì²­í¬: {len(relevant_chunks)}ê°œ")
    logger.info(f"ì›ë³¸ ë…¼ë¬¸: {len(papers)}ê°œ")
    
    if not relevant_chunks and not papers:
        logger.warning("ìš”ì•½í•  ë…¼ë¬¸ì´ ì—†ìŒ")
        return {
            "summarized_content": "ìš”ì•½í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.",
            "react_steps": [ReActStep(
                step_type="observation",
                content="ìš”ì•½í•  ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."
            )]
        }
    
    try:
        llm = get_llm()
        
        action_step = ReActStep(
            step_type="action",
            content=f"ê´€ë ¨ ë…¼ë¬¸ë“¤ì˜ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."
        )
        
        seen_papers = {}
        for chunk in relevant_chunks[:10]:
            arxiv_id = chunk.get('arxiv_id')
            if arxiv_id and arxiv_id not in seen_papers:
                seen_papers[arxiv_id] = {
                    'title': chunk.get('title', ''),
                    'abstract': chunk.get('content', ''),
                    'authors': chunk.get('authors', ''),
                }
        
        logger.info(f"ì¶”ì¶œëœ ë…¼ë¬¸: {len(seen_papers)}ê°œ")
        
        summary_parts = []
        
        for arxiv_id, paper_info in list(seen_papers.items())[:5]:
            try:
                if paper_info['title']:
                    prompt = SUMMARIZE_PROMPT.format(
                        title=paper_info['title'],
                        abstract=paper_info['abstract'][:500]
                    )
                    
                    response = llm.invoke([
                        HumanMessage(content=prompt)
                    ])
                    
                    summary_parts.append(f"\n## {paper_info['title']}\n\n{response.content}")
                    logger.debug(f"âœ“ {paper_info['title'][:30]}... ìš”ì•½ ì™„ë£Œ")
                    
            except Exception as e:
                logger.warning(f"ë…¼ë¬¸ ìš”ì•½ ì‹¤íŒ¨: {str(e)}")
                summary_parts.append(f"\n## {paper_info['title']}\n\nìš”ì•½ ìƒì„± ì‹¤íŒ¨")
        
        summarized_content = "\n".join(summary_parts) if summary_parts else "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        observation_step = ReActStep(
            step_type="observation",
            content=f"{len(summary_parts)}ê°œ ë…¼ë¬¸ì˜ ìš”ì•½ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
        )
        
        logger.info(f"âœ“ {len(summary_parts)}ê°œ ìš”ì•½ ìƒì„± ì™„ë£Œ")
        
        return {
            "summarized_content": summarized_content,
            "react_steps": [action_step, observation_step]
        }
        
    except Exception as e:
        logger.error(f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return {
            "summarized_content": f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}",
            "react_steps": [ReActStep(step_type="observation", content=f"ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {str(e)}")]
        }


# ============================================
# ë…¸ë“œ 10: ìµœì¢… ì‘ë‹µ ìƒì„± (generate_response_node)
# ============================================

FINAL_RESPONSE_PROMPT = """
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

## ì‚¬ìš©ì ì§ˆë¬¸
{question}

## ê²€ìƒ‰ëœ ì •ë³´
{papers_info}

## ìš”ì•½ëœ ë…¼ë¬¸
{summarized_content}

## ë‹µë³€ ì‘ì„± ì§€ì¹¨
1. ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€ìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”.
2. ê´€ë ¨ ì—°êµ¬ ë™í–¥ì„ ì„¤ëª…í•˜ì„¸ìš”.
3. ê²€ìƒ‰ëœ ë…¼ë¬¸ë“¤ì˜ ì£¼ìš” ë‚´ìš©ì„ ì¸ìš©í•˜ë©´ì„œ ì„¤ëª…í•˜ì„¸ìš”.
4. ì¶”ê°€ í•™ìŠµì´ë‚˜ íƒêµ¬ë¥¼ ìœ„í•œ ì œì•ˆì„ ì œì‹œí•˜ì„¸ìš”.
5. í•œêµ­ì–´ë¡œ ìì„¸í•˜ê³  ì „ë¬¸ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.

ë‹µë³€ì˜ ê¸¸ì´: 500-1500 ê¸€ì
"""


def generate_response_node(state: AgentState) -> dict:
    """
    ê²€ìƒ‰ ê²°ê³¼ì™€ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    user_question = state.get("user_question", "")
    relevant_chunks = state.get("relevant_chunks", [])
    summarized_content = state.get("summarized_content", "")
    error_message = state.get("error_message")
    
    logger.info("="*60)
    logger.info("[GENERATE_RESPONSE] ìµœì¢… ì‘ë‹µ ìƒì„±")
    logger.info("="*60)
    
    if error_message:
        logger.warning(f"ì´ì „ ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {error_message}")
        return {
            "final_response": f"""ì£„ì†¡í•˜ì§€ë§Œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

ì˜¤ë¥˜ ë‚´ìš©: {error_message}

ë‹¤ìŒì„ ì‹œë„í•´ì£¼ì„¸ìš”:
1. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”
2. ê²€ìƒ‰í•  ë…¼ë¬¸ ìˆ˜ë¥¼ ì¤„ì—¬ë³´ì„¸ìš”
3. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”""",
            "is_complete": True
        }
    
    if not relevant_chunks:
        logger.warning("ê´€ë ¨ ì²­í¬ê°€ ì—†ìŒ")
        return {
            "final_response": """ì…ë ¥í•˜ì‹  ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë…¼ë¬¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

ë‹¤ìŒì„ ì‹œë„í•´ì£¼ì„¸ìš”:
1. ë” ì¼ë°˜ì ì¸ í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
2. ê²€ìƒ‰í•  ë…¼ë¬¸ ìˆ˜ë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš”
3. ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ì§ˆë¬¸ì„ ë‹¤ì‹œ ì‘ì„±í•˜ì„¸ìš”""",
            "is_complete": True
        }
    
    try:
        llm = get_llm()
        
        papers_info = ""
        for i, chunk in enumerate(relevant_chunks[:5], 1):
            papers_info += f"\n\n{i}. {chunk['title']}\n"
            papers_info += f"ìœ ì‚¬ë„: {chunk['similarity_score']:.2%}\n"
            papers_info += f"ë‚´ìš©: {chunk['content'][:150]}..."
        
        prompt = FINAL_RESPONSE_PROMPT.format(
            question=user_question,
            papers_info=papers_info,
            summarized_content=summarized_content[:500]
        )
        
        logger.info("LLMì— ìµœì¢… ì‘ë‹µ ìš”ì²­...")
        
        response = llm.invoke([
            SystemMessage(content="ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ í•™ìˆ  ì—°êµ¬ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
            HumanMessage(content=prompt)
        ])
        
        final_response = response.content
        
        logger.info("âœ“ ìµœì¢… ì‘ë‹µ ìƒì„± ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"ìµœì¢… ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        final_response = f"""ê²€ìƒ‰ëœ {len(relevant_chunks)}ê°œì˜ ê´€ë ¨ ì²­í¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.

{summarized_content}

ì˜¤ë¥˜ë¡œ ì¸í•´ ìƒì„¸í•œ ë¶„ì„ì„ ì™„ì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {str(e)}"""
    
    decision_step = ReActStep(
        step_type="thought",
        content="ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
    )
    
    return {
        "final_response": final_response,
        "is_complete": True,
        "react_steps": [decision_step]
    }