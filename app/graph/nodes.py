# -*- coding: utf-8 -*-
"""
ìˆ˜ì •ëœ LangGraph ë…¸ë“œë“¤ (ë‹¨ìˆœí™” ë²„ì „)

ì£¼ìš” ë³€ê²½ì‚¬í•­:
- analyze_question_nodeì—ì„œ ìë™ ìŠ¹ì¸ ë¡œì§ ì œê±°
- ì¬ë¶„ì„ì´ë“  ì•„ë‹ˆë“  í•­ìƒ ì‚¬ìš©ìì—ê²Œ í‚¤ì›Œë“œ í™•ì¸ ìš”ì²­
- is_reanalyzing í”Œë˜ê·¸ëŠ” ìœ ì§€í•˜ë˜, ë©”ì‹œì§€ í‘œì‹œìš©ìœ¼ë¡œë§Œ ì‚¬ìš©
"""

import logging
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

from app.graph.state import (
    AgentState, 
    ReActStep, 
    InterruptData
)
from app.tools.paper_search.arxiv_tool import search_arxiv
from app.config import get_settings

settings = get_settings()
logger = logging.getLogger(__name__)


def get_llm(model: str = None):
    """LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return ChatOpenAI(
        model=model or settings.default_model,
        api_key=settings.openai_api_key,
        temperature=0.3
    )


# ============================================
# ë…¸ë“œ 1: ì§ˆë¬¸ ìˆ˜ì‹ 
# ============================================

def receive_question_node(state: AgentState) -> dict:
    """ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ìˆ˜ì‹ í•˜ê³  ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
    user_question = state.get("user_question", "")
    
    logger.info("="*60)
    logger.info("[RECEIVE_QUESTION] ì‚¬ìš©ì ì§ˆë¬¸ ìˆ˜ì‹ ")
    logger.info("="*60)
    logger.info(f"ì§ˆë¬¸: {user_question}")
    
    thought_content = f'ì‚¬ìš©ì ì§ˆë¬¸ì„ ìˆ˜ì‹ í–ˆìŠµë‹ˆë‹¤: "{user_question}"\nì´ì œ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ í‚¤ì›Œë“œì™€ ì˜ë„ë¥¼ íŒŒì•…í•´ì•¼ í•©ë‹ˆë‹¤.'
    
    new_step = ReActStep(
        step_type="thought",
        content=thought_content
    )
    
    return {
        "react_steps": [new_step]
    }


# ============================================
# ë…¸ë“œ 2: ì§ˆë¬¸ ë¶„ì„ (ë‹¨ìˆœí™” ë²„ì „)
# ============================================

QUESTION_ANALYSIS_PROMPT = """
ë‹¹ì‹ ì€ í•™ìˆ  ì—°êµ¬ ì§ˆë¬¸ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

## ì‚¬ìš©ì ì§ˆë¬¸
{question}

## ë¶„ì„í•´ì•¼ í•  í•­ëª©

1. **í•µì‹¬ í‚¤ì›Œë“œ**: ë…¼ë¬¸ ê²€ìƒ‰ì— ì‚¬ìš©í•  í•µì‹¬ ê¸°ìˆ  í‚¤ì›Œë“œ 2-5ê°œ
   - ì˜ì–´ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”
   - êµ¬ì²´ì ì´ê³  ê²€ìƒ‰ íš¨ê³¼ê°€ ì¢‹ì€ í‚¤ì›Œë“œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”

2. **ì§ˆë¬¸ ì˜ë„**: ì‚¬ìš©ìê°€ ì•Œê³  ì‹¶ì–´í•˜ëŠ” ê²ƒì´ ë¬´ì—‡ì¸ì§€
   - "ìµœì‹  ì—°êµ¬ ë™í–¥" / "íŠ¹ì • ê¸°ìˆ  ì„¤ëª…" / "ë¹„êµ ë¶„ì„" / "ì‘ìš© ì‚¬ë¡€" ë“±

3. **ì—°êµ¬ ë„ë©”ì¸**: ì–´ë–¤ í•™ë¬¸ ë¶„ì•¼ì— í•´ë‹¹í•˜ëŠ”ì§€
   - "computer science" / "physics" / "mathematics" / "biology" ë“±

## ì‘ë‹µ í˜•ì‹ (ë°˜ë“œì‹œ ì´ í˜•ì‹ì„ ë”°ë¼ì£¼ì„¸ìš”)
KEYWORDS: keyword1, keyword2, keyword3
INTENT: ì§ˆë¬¸ ì˜ë„ ì„¤ëª…
DOMAIN: ì—°êµ¬ ë„ë©”ì¸
"""


def analyze_question_node(state: AgentState) -> dict:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    ğŸ”‘ ë‹¨ìˆœí™”: ì¬ë¶„ì„ì´ë“  ì•„ë‹ˆë“  í•­ìƒ ì‚¬ìš©ìì—ê²Œ í™•ì¸ ìš”ì²­
    """
    
    user_question = state.get("user_question", "")
    is_reanalyzing = state.get("is_reanalyzing", False)
    
    logger.info("="*60)
    logger.info("[ANALYZE_QUESTION] ì§ˆë¬¸ ë¶„ì„ ì‹œì‘")
    logger.info(f"  ì¬ë¶„ì„ ëª¨ë“œ: {is_reanalyzing}")
    logger.info("="*60)
    logger.info(f"ë¶„ì„ ëŒ€ìƒ: {user_question[:50]}...")
    
    try:
        llm = get_llm(settings.light_model)
        prompt = QUESTION_ANALYSIS_PROMPT.format(question=user_question)
        
        logger.info("LLMì— ì§ˆë¬¸ ë¶„ì„ ìš”ì²­ ì „ì†¡...")
        
        response = llm.invoke([
            SystemMessage(content="ë‹¹ì‹ ì€ í•™ìˆ  ì—°êµ¬ ì§ˆë¬¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
            HumanMessage(content=prompt)
        ])
        
        logger.info("âœ“ LLM ì‘ë‹µ ìˆ˜ì‹ ")
        
        # LLM ì‘ë‹µ íŒŒì‹±
        response_text = response.content
        keywords = []
        intent = ""
        domain = ""
        
        for line in response_text.strip().split('\n'):
            line = line.strip()
            if line.startswith("KEYWORDS:"):
                keywords_str = line.replace("KEYWORDS:", "").strip()
                keywords = [k.strip() for k in keywords_str.split(",") if k.strip()]
            elif line.startswith("INTENT:"):
                intent = line.replace("INTENT:", "").strip()
            elif line.startswith("DOMAIN:"):
                domain = line.replace("DOMAIN:", "").strip()
        
        logger.info(f"âœ“ ë¶„ì„ ì™„ë£Œ")
        logger.info(f"  ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
        logger.info(f"  ì§ˆë¬¸ ì˜ë„: {intent}")
        logger.info(f"  ì—°êµ¬ ë„ë©”ì¸: {domain}")
        
        # ğŸ”‘ ë‹¨ìˆœí™”: ì¬ë¶„ì„ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ ë™ì¼í•œ ë©”ì‹œì§€
        if is_reanalyzing:
            observation_content = f"""ì§ˆë¬¸ ì¬ë¶„ì„ ì™„ë£Œ:
- ìƒˆë¡œìš´ í‚¤ì›Œë“œ: {', '.join(keywords)}
- ì§ˆë¬¸ ì˜ë„: {intent}
- ì—°êµ¬ ë„ë©”ì¸: {domain}

ìƒˆë¡œìš´ í‚¤ì›Œë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."""
            logger.info("  â†’ ì¬ë¶„ì„ ì™„ë£Œ: ì‚¬ìš©ì í™•ì¸ ëŒ€ê¸°")
        else:
            observation_content = f"""ì§ˆë¬¸ ë¶„ì„ ì™„ë£Œ:
- ì¶”ì¶œëœ í‚¤ì›Œë“œ: {', '.join(keywords)}
- ì§ˆë¬¸ ì˜ë„: {intent}
- ì—°êµ¬ ë„ë©”ì¸: {domain}"""
        
        new_step = ReActStep(
            step_type="observation",
            content=observation_content
        )
        
        # ğŸ”‘ í•µì‹¬ ìˆ˜ì •: ìë™ ìŠ¹ì¸ ì œê±°, í•­ìƒ Noneìœ¼ë¡œ ì„¤ì •
        return {
            "extracted_keywords": keywords,
            "question_intent": intent,
            "question_domain": domain,
            "is_reanalyzing": False,  # í”Œë˜ê·¸ ì´ˆê¸°í™”
            "keyword_confirmation_response": None,  # í•­ìƒ ì‚¬ìš©ì í™•ì¸ í•„ìš”
            "react_steps": [new_step]
        }
        
    except Exception as e:
        logger.error(f"ì§ˆë¬¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        return {
            "extracted_keywords": ["research"],
            "question_intent": "general research",
            "question_domain": "computer science",
            "is_reanalyzing": False,
            "keyword_confirmation_response": None,
            "error_message": str(e),
            "react_steps": [ReActStep(step_type="observation", content=f"ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {str(e)}")]
        }


# ============================================
# ë…¸ë“œ 3: í‚¤ì›Œë“œ í™•ì¸ ìš”ì²­
# ============================================

def request_keyword_confirmation_node(state: AgentState) -> dict:
    """
    ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ë§ëŠ”ì§€ ì‚¬ìš©ìì—ê²Œ í™•ì¸ë°›ìŠµë‹ˆë‹¤.
    
    ì²« ë²ˆì§¸ Human-in-the-Loop Interrupt ì§€ì ì…ë‹ˆë‹¤.
    """
    
    keywords = state.get("extracted_keywords", [])
    
    logger.info("[REQUEST_KEYWORD_CONFIRMATION] ì‚¬ìš©ì í™•ì¸ ëŒ€ê¸° ì‹œì‘")
    
    message = f"""
ì¶”ì¶œëœ í‚¤ì›Œë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.

í‚¤ì›Œë“œ: {', '.join(keywords) if keywords else 'ì—†ìŒ'}

ë§ìœ¼ë©´ "í™•ì¸"ì„, ìˆ˜ì •ì´ í•„ìš”í•˜ë©´ "ë‹¤ì‹œ"ë¼ê³  ì…ë ¥í•´ì£¼ì„¸ìš”.
    """.strip()
    
    interrupt_data = InterruptData(
        interrupt_type="confirm_keywords",
        message=message,
        options=["í™•ì¸", "ë‹¤ì‹œ"],
        default_value="í™•ì¸",
        metadata={
            "keywords": keywords,
            "stage": 1
        }
    )
    
    thought_content = "í‚¤ì›Œë“œ ì¶”ì¶œì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ í™•ì¸ì„ ìš”ì²­í•©ë‹ˆë‹¤."
    
    new_step = ReActStep(
        step_type="thought",
        content=thought_content
    )
    
    return {
        "interrupt_data": interrupt_data,
        "waiting_for": "keyword_confirmation",
        "interrupt_stage": 1,
        "waiting_for_user": True,
        "react_steps": [new_step]
    }


# ============================================
# ë…¸ë“œ 4: í‚¤ì›Œë“œ í™•ì¸ ì‘ë‹µ ì²˜ë¦¬
# ============================================

def process_keyword_confirmation_response_node(state: AgentState) -> dict:
    """
    ì‚¬ìš©ìì˜ í‚¤ì›Œë“œ í™•ì¸ ì‘ë‹µì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    - "ë‹¤ì‹œ" â†’ is_reanalyzing=True ì„¤ì •, ì§ˆë¬¸ ë¶„ì„ ë‹¨ê³„ë¡œ ëŒì•„ê°
    - ê·¸ ì™¸ ("í™•ì¸" ë“±) â†’ is_reanalyzing=False, ë…¼ë¬¸ ìˆ˜ ì„ íƒ ë‹¨ê³„ë¡œ ì§„í–‰
    """
    
    user_response = state.get("user_response", "").strip().lower()
    
    logger.info("[PROCESS_KEYWORD_CONFIRMATION] ì‚¬ìš©ì ì‘ë‹µ ì²˜ë¦¬")
    logger.info(f"  ì‘ë‹µ: {user_response}")
    
    # "ë‹¤ì‹œ" ì‘ë‹µ í™•ì¸
    if user_response in ["ë‹¤ì‹œ", "retry", "ë‹¤ì‹œí•˜ê¸°", "ìˆ˜ì •", "ë‹¤ì‹œí•´", "reanalyze"]:
        logger.info("  â†’ 'ë‹¤ì‹œ' ì„ íƒ: ì¬ë¶„ì„ ëª¨ë“œ í™œì„±í™”")
        
        observation_content = "ì‚¬ìš©ìê°€ í‚¤ì›Œë“œ ì¬ë¶„ì„ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ ë¶„ì„í•©ë‹ˆë‹¤."
        
        new_step = ReActStep(
            step_type="observation",
            content=observation_content
        )
        
        return {
            "keyword_confirmation_response": "retry",
            "is_reanalyzing": True,  # ì¬ë¶„ì„ ëª¨ë“œ í™œì„±í™”
            "waiting_for": None,
            "waiting_for_user": False,
            "interrupt_data": None,
            "react_steps": [new_step],
            "user_response": None
        }
    
    # ê·¸ ì™¸ì˜ ê²½ìš° "í™•ì¸"ìœ¼ë¡œ ì²˜ë¦¬
    logger.info("  â†’ 'í™•ì¸' ì„ íƒ: ë…¼ë¬¸ ìˆ˜ ì„ íƒ ë‹¨ê³„ë¡œ ì´ë™")
    
    observation_content = f"ì‚¬ìš©ìê°€ í‚¤ì›Œë“œë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤. í‚¤ì›Œë“œ: {', '.join(state.get('extracted_keywords', []))}"
    
    new_step = ReActStep(
        step_type="observation",
        content=observation_content
    )
    
    return {
        "keyword_confirmation_response": "confirmed",
        "is_reanalyzing": False,
        "waiting_for": None,
        "waiting_for_user": False,
        "interrupt_data": None,
        "interrupt_stage": 1,
        "react_steps": [new_step],
        "user_response": None
    }


# ============================================
# ë…¸ë“œ 5: ë…¼ë¬¸ ìˆ˜ ì„ íƒ ìš”ì²­
# ============================================

def request_paper_count_node(state: AgentState) -> dict:
    """ëª‡ ê°œì˜ ë…¼ë¬¸ì„ ê²€ìƒ‰í• ì§€ ì‚¬ìš©ìì—ê²Œ ì„ íƒë°›ìŠµë‹ˆë‹¤."""
    
    logger.info("[REQUEST_PAPER_COUNT] ì‚¬ìš©ì ì„ íƒ ëŒ€ê¸° ì‹œì‘")
    
    message = """
ê²€ìƒ‰í•  ë…¼ë¬¸ì˜ ê°œìˆ˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.

1ë¶€í„° 10 ì‚¬ì´ì˜ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.
(ê¸°ë³¸ê°’: 3ê°œ)

ë” ë§ì€ ë…¼ë¬¸ì„ ì„ íƒí• ìˆ˜ë¡ ì²˜ë¦¬ ì‹œê°„ì´ ê¸¸ì–´ì§‘ë‹ˆë‹¤.
    """.strip()
    
    interrupt_data = InterruptData(
        interrupt_type="select_paper_count",
        message=message,
        options=["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"],
        default_value="3",
        metadata={
            "stage": 2
        }
    )
    
    thought_content = "í‚¤ì›Œë“œ í™•ì¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ê²€ìƒ‰í•  ë…¼ë¬¸ ìˆ˜ë¥¼ ì‚¬ìš©ìì—ê²Œ ì„ íƒë°›ìŠµë‹ˆë‹¤."
    
    new_step = ReActStep(
        step_type="thought",
        content=thought_content
    )
    
    return {
        "interrupt_data": interrupt_data,
        "waiting_for": "paper_count_selection",
        "interrupt_stage": 2,
        "waiting_for_user": True,
        "react_steps": [new_step]
    }


# ============================================
# ë…¸ë“œ 6: ë…¼ë¬¸ ìˆ˜ ì‘ë‹µ ì²˜ë¦¬
# ============================================

def process_paper_count_response_node(state: AgentState) -> dict:
    """ì‚¬ìš©ìê°€ ì„ íƒí•œ ë…¼ë¬¸ ìˆ˜ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    
    user_response = state.get("user_response", "3")
    
    logger.info("[PROCESS_PAPER_COUNT] ì‚¬ìš©ì ì‘ë‹µ ì²˜ë¦¬")
    logger.info(f"  ì‘ë‹µ: {user_response}")
    
    try:
        paper_count = int(user_response)
        paper_count = max(1, min(10, paper_count))
        logger.info(f"  â†’ í•´ì„ë¨: {paper_count}ê°œ")
    except ValueError:
        logger.warning(f"  â†’ ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥, ê¸°ë³¸ê°’ 3 ì‚¬ìš©")
        paper_count = 3
    
    observation_content = f"ì‚¬ìš©ìê°€ ë…¼ë¬¸ ìˆ˜ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤: {paper_count}ê°œ"
    
    new_step = ReActStep(
        step_type="observation",
        content=observation_content
    )
    
    return {
        "paper_count": paper_count,
        "waiting_for": None,
        "waiting_for_user": False,
        "interrupt_data": None,
        "interrupt_stage": 2,
        "react_steps": [new_step],
        "user_response": None
    }


# ============================================
# ë…¸ë“œ 7-10: ë‚˜ë¨¸ì§€ ë…¸ë“œë“¤ì€ ê¸°ì¡´ê³¼ ë™ì¼
# ============================================

def search_papers_node(state: AgentState) -> dict:
    """arXivì—ì„œ ë…¼ë¬¸ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    # ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼
    # ì—¬ê¸°ì„œëŠ” ê°„ëµí•˜ê²Œ í‘œì‹œ
    return {
        "papers": [],
        "error_message": "search_papers_node êµ¬í˜„ í•„ìš”"
    }


def evaluate_relevance_node(state: AgentState) -> dict:
    """ì˜ë¯¸ ê¸°ë°˜ ê´€ë ¨ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤."""
    return {
        "relevant_chunks": [],
        "error_message": "evaluate_relevance_node êµ¬í˜„ í•„ìš”"
    }


def summarize_papers_node(state: AgentState) -> dict:
    """ë…¼ë¬¸ì„ ìš”ì•½í•©ë‹ˆë‹¤."""
    return {
        "summarized_content": "summarize_papers_node êµ¬í˜„ í•„ìš”"
    }


def generate_response_node(state: AgentState) -> dict:
    """ìµœì¢… ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    return {
        "final_response": "generate_response_node êµ¬í˜„ í•„ìš”",
        "is_complete": True
    }